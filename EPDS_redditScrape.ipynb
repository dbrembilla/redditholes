{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping posts from r/conspiracy\n",
    "\n",
    "Extraction of all the posts resent in r/conspiracy, with title, content and url.\n",
    "First of all we need to import praw and access reddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import pandas as pd\n",
    "reddit = praw.Reddit(client_id = \"lfdF-I0nP51611Fidk3OQw\", client_secret= \"MShIIjVvaY-WZIdvvB-qTCWZ9dPhZw\", user_agent= \"epds_project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now access the subreddit, extract all the posts url in it and check which other subreddit reposted that posts and how much:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "post_list=list()\n",
    "subreddit_list = list()\n",
    "conspiracy_dict=dict()\n",
    "\n",
    "for i in reddit.subreddit(\"conspiracy\").top(limit=5000): #recupera i 5000 post top di conspiracy\n",
    "    post_list.append((i.title, i.score, i.url)) #recupera titolo, numero di upvote + il permalink\n",
    "\n",
    "\n",
    "for post in post_list:\n",
    "    for repost in reddit.subreddit('all').search('url:'+post[2]): #trova quando il post Ã¨ stato citato\n",
    "        subreddit_url = str(repost.subreddit)\n",
    "        subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "        if subreddit_url in conspiracy_dict.keys():\n",
    "            conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "            conspiracy_dict[subreddit_url][1][0] +=1\n",
    "        else:\n",
    "            conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "            conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "            \n",
    "                \n",
    "\n",
    "\n",
    "df = pd.DataFrame(conspiracy_dict)\n",
    "df.to_csv(r'results/conspiracy_data/conspiracy_top_url.csv',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the network's first level:\n",
    "Reading the csv we can now look for posts that have the same url as the ones in the file: this is done in order to later create a network where the subreddits will serve as nodes and the crossposts as edges between them (the more the crossposts the wider the edge).\n",
    "First we need to look at all the files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_in_dir(dir):\n",
    "    for filename in os.listdir(dir):\n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f) and f[-4:] == \".csv\":\n",
    "            yield f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can scrape stuff from reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/conspiracy_data/conspiracy_top_url.csv')\n",
    "id_to_analyse = []\n",
    "for column in df1.columns:\n",
    "    value = df1[column][1][1:-1]\n",
    "    if int(value) >= 5:\n",
    "        ind = column.index('/r/')\n",
    "        id = column[ind+3:]\n",
    "        id_to_analyse.append(id)\n",
    "        \n",
    "\n",
    "for subreddit in id_to_analyse:\n",
    "    post_list=list()\n",
    "    subreddit_list = list()\n",
    "    conspiracy_dict=dict() \n",
    "    for i in reddit.subreddit(subreddit).top(limit=5000):\n",
    "        post_list.append((i.title, i.score, i.url))\n",
    "\n",
    "\n",
    "\n",
    "    for post in post_list:\n",
    "        for repost in reddit.subreddit('all').search('url:'+post[2]):\n",
    "            if repost.subreddit_id != \"t5_\"+reddit.subreddit(subreddit).id: #cosa facciamo?\n",
    "                subreddit_url = str(repost.subreddit)\n",
    "                subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "                if subreddit_url in conspiracy_dict.keys():\n",
    "                    conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                    conspiracy_dict[subreddit_url][1][0] +=1\n",
    "                else:\n",
    "                    conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "                    conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "    df = pd.DataFrame(conspiracy_dict)\n",
    "    df.to_csv(r'results/1st_level/'+subreddit+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the second level\n",
    "Now we need to iterate again on the subreddit we found in order to get where this path, started from conspiracy will led.\\\n",
    "We also need to create a function to dump done csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\GIORGI~1\\AppData\\Local\\Temp/ipykernel_23908/523188709.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m                                         \u001b[1;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpost_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                                                 \u001b[1;32mfor\u001b[0m \u001b[0mrepost\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'url:'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                                                         \u001b[1;32mif\u001b[0m \u001b[0mrepost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubreddit_id\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"t5_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mreddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#cosa facciamo?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m                                                                 \u001b[0msubreddit_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                                                                 \u001b[0msubreddit_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.reddit.com/r/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msubreddit_url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\praw\\models\\reddit\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attribute)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;34m\"\"\"Return the value of `attribute`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetched\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         raise AttributeError(\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\praw\\models\\reddit\\subreddit.py\u001b[0m in \u001b[0;36m_fetch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\praw\\models\\reddit\\subreddit.py\u001b[0m in \u001b[0;36m_fetch_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAPI_PATH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GET\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\praw\\reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, path, params, data, files, json)\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mClientException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"At most one of `data` and `json` is supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m             return self._core.request(\n\u001b[0m\u001b[0;32m    886\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m             \u001b[0mjson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"api_type\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moauth_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         return self._request_with_retries(\n\u001b[0m\u001b[0;32m    331\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mretry_strategy_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         response, saved_exception = self._make_request(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[0;32m    183\u001b[0m     ):\n\u001b[0;32m    184\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             response = self._rate_limiter.call(\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_header_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\prawcore\\rate_limit.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \"\"\"\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"headers\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset_header_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\prawcore\\rate_limit.py\u001b[0m in \u001b[0;36mdelay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"Sleeping: {sleep_seconds:0.2f} seconds prior to call\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msleep_seconds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse_headers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "top_comments_list = dict()\n",
    "to_scan = [file for file in get_all_in_dir(\"results/1st_level_2\")]\n",
    "to_avoid= [file.split('\\\\')[1] for file in get_all_in_dir(\"results/1st_level\")]\n",
    "to_avoid.extend(file.split('\\\\')[1] for file in get_all_in_dir(\"results/1st_level_2\"))\n",
    "to_avoid.extend(file.split('\\\\')[1] for file in get_all_in_dir(\"results/2nd_level\"))\n",
    "\n",
    "for subr in to_scan:\n",
    "        id_to_analyse = []\n",
    "        print(f'now opening {subr}')\n",
    "        try:\n",
    "                df1 = pd.read_csv(subr)\n",
    "        except:\n",
    "                pass\n",
    "        try:\n",
    "                df1 = pd.read_csv(subr, encoding='utf8')\n",
    "        except:\n",
    "                print(f'unable to open {subr}')\n",
    "                continue\n",
    "        for column in df1.columns:\n",
    "                try: \n",
    "                        value = df1[column][1][1:-1]\n",
    "                except:\n",
    "                        continue\n",
    "                if int(value) >= 5:\n",
    "                        ind = column.index('/r/')\n",
    "                        id = column[ind+3:]\n",
    "                        if not id + '.csv' in to_avoid:\n",
    "                                try:\n",
    "                                        post_list=list()\n",
    "                                        subreddit_list = list()\n",
    "                                        conspiracy_dict=dict() \n",
    "                                        for i in reddit.subreddit(id).top(limit=500):\n",
    "                                                post_list.append((i.title, i.score, i.url))\n",
    "\n",
    "                                        for post in post_list:\n",
    "                                                for repost in reddit.subreddit('all').search('url:'+post[2]):\n",
    "                                                        if repost.subreddit_id != \"t5_\"+reddit.subreddit(id).id: #cosa facciamo?\n",
    "                                                                subreddit_url = str(repost.subreddit)\n",
    "                                                                subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "                                                        else:\n",
    "                                                                continue\n",
    "                                                        if subreddit_url in conspiracy_dict.keys():\n",
    "                                                                conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                                                                conspiracy_dict[subreddit_url][1][0] +=1\n",
    "                                                        else:\n",
    "                                                                conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "                                                                conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                                        \n",
    "                                        df = pd.DataFrame(conspiracy_dict)\n",
    "                                        df.to_csv(r'results/2nd_level/'+id+'.csv')\n",
    "                                        with open(\"results/2nd_level/done_2.txt\",'a', encoding = \"utf-8\") as text_note:\n",
    "                                                text_note.write(id + \"\\n\")\n",
    "                                                text_note.close()\n",
    "                                        \n",
    "                                except Exception as E:\n",
    "                                        print(E)\n",
    "                                        with open(\"results/2nd_level/error_2.txt\",'a', encoding = \"utf-8\") as text_note:\n",
    "                                                text_note.write(id + \"\\n\")\n",
    "                                                text_note.close()\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data\n",
    "In order to perform more efficiently the operations of data representation we decided two perform two operations:\n",
    "- 1st we removed all columns belonging to subreddits with an amount of crossposts inferior to 5\n",
    "- 2nd we decided to remove the  [ ] for the number of crossopst between channels\n",
    "\n",
    "These was in order to have cleaner and more usable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing parentheses and connections with less than 5 reposts\n",
    "\n",
    "datasets= [file for file in get_all_in_dir(\"results/1st_level\")]\n",
    "datasets.extend(file for file in get_all_in_dir(\"results/1st_level_2\"))\n",
    "datasets.extend(file for file in get_all_in_dir(\"results/2nd_level\"))\n",
    "datasets.extend(\"results/conspiracy_data/conspiracy_top_url.csv\")\n",
    "\n",
    "for file in datasets:\n",
    "    df = pd.read_csv(file)\n",
    "    for col in df.columns:\n",
    "        if \"u/\" in col:\n",
    "            df.drop(col, inplace=True, axis=1)\n",
    "        elif isinstance(df[col][1], str):\n",
    "            df[col][1]=int(df[col][1][1:-1])\n",
    "            if df[col][1]<5:\n",
    "                df.drop(col, inplace=True, axis=1)\n",
    "        else:\n",
    "            df.drop(col, inplace=True, axis=1)\n",
    "    #df.to_csv(file, encoding=\"utf8\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOBBIAMO ARRIVARE AD AVERE UN PERMALINK ASSOCIATO ALLA SENTIMENT DEI COMMENTI E DEL POST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "To extract topics discussed directly from posts in reddit we decided to rely on Gensim, a package runnable in python that is comparable to Mallet in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['re', 'edit']) \n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as genmodels  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def iterSample(iterable, samplesize):\n",
    "    results = []\n",
    "    for i, v in enumerate(iterable):\n",
    "        r = random.randint(0, i)\n",
    "        if r < samplesize:\n",
    "            if i < samplesize:\n",
    "                results.insert(r, v) # add first samplesize items in random order\n",
    "            else:\n",
    "                results[r] = v # at a decreasing rate, replace random items\n",
    "\n",
    "    if len(results) < samplesize:\n",
    "        raise ValueError(\"Sample larger than population.\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to prepare the stopwords and import the datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "dir = \"results/1st_level\"\n",
    "top_comments_list = dict()\n",
    "to_scan = [] #iterate over this\n",
    "for filename in os.listdir(dir):\n",
    "    f = os.path.join(dir, filename)\n",
    "    if os.path.isfile(f) and f[-4:] == \".csv\":\n",
    "        to_scan.append(f) \n",
    "for subreddit in to_scan:\n",
    "        try:\n",
    "                df = pd.read_csv(subreddit, sep=';', encoding='unicode_escape')\n",
    "        except:\n",
    "                pass\n",
    "        try:\n",
    "                df = pd.read_csv(subreddit, sep=',', encoding='utf8')\n",
    "        except:\n",
    "                pass\n",
    "        try:\n",
    "                df = pd.read_csv(subreddit, sep=';', encoding='utf8')\n",
    "        except:\n",
    "                pass\n",
    "        try:\n",
    "                df = pd.read_csv(subreddit, sep=',', encoding='unicode_escape')\n",
    "        except:\n",
    "                print(f'{subreddit} should be opened with different settings.')\n",
    "                continue\n",
    "        for column in df.columns: # qui inserire un loop sui permalink\n",
    "                list_post=[]\n",
    "                cross_counter = column[1]\n",
    "                try:\n",
    "                        list_post= df[column][0].split(',')\n",
    "                except:\n",
    "                        try:\n",
    "                                continue\n",
    "                        except:\n",
    "                                print(f'{subreddit} has problems in formatting.')\n",
    "                                pass\n",
    "                for post_url in list_post:\n",
    "                        tmp = post_url \n",
    "                        tmp =  re.sub(\"'[]\", '', tmp)\n",
    "                        \n",
    "                        try:\n",
    "                                post = reddit.submission(url=tmp)\n",
    "                        except: # some posts may be removed\n",
    "                                continue\n",
    "                        try:\n",
    "                                post.comments.replace_more(limit=0)\n",
    "                        except:\n",
    "                                print(f'{tmp} was not accessible')\n",
    "                                continue\n",
    "                        result = []\n",
    "                        if len(post.comments)> 500:\n",
    "                                for comment in iterSample(post.comments, 500):\n",
    "                                        result.append(comment)\n",
    "                        else:\n",
    "                                for comment in post.comments:\n",
    "                                        result.append(comment)\n",
    "\n",
    "                        name= subreddit + \" @ \" + column + \" <\" + cross_counter + \">\"            \n",
    "\n",
    "                        if name in top_comments_list.keys():\n",
    "                                top_comments_list[name].extend(result)\n",
    "                        else:\n",
    "                                top_comments_list[name]=result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform the topic modeling: first we need to define the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print a csv for each crossposting between subreddits with the topic modeling of the common posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for key in top_comments_list.keys():\n",
    "\n",
    "    # Convert to list\n",
    "    data = top_comments_list[key]\n",
    "\n",
    "    # Remove tags from comments\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent.body) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "    # Tokenizing the text\n",
    "    data_words = list(sent_to_words(data))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Human readable format of corpus (term-frequency)\n",
    "    [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "    if corpus != [] and corpus != [[]]:\n",
    "        # Build LDA model\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                id2word=id2word,\n",
    "                                                num_topics=10, \n",
    "                                                random_state=100,\n",
    "                                                update_every=1,\n",
    "                                                chunksize=100,\n",
    "                                                passes=10,\n",
    "                                                alpha='auto',\n",
    "                                                per_word_topics=True)\n",
    "\n",
    "        # Print the Keyword in the 10 topics\n",
    "        df_topic = pd.DataFrame(lda_model.print_topics())\n",
    "        df_topic.to_csv(r\"results/topic_models/1st_level\"+ key)\n",
    "\n",
    "        #pprint(lda_model.print_topics())\n",
    "\n",
    "        doc_lda = lda_model[corpus]\n",
    "    else: \n",
    "        print (key + \" is done.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69eb92836b941e979072a76c7fcfffe5419cca933cedd02cfafbdfca1a93358c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
