{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping posts from r/conspiracy\n",
    "\n",
    "Extraction of all the posts resent in r/conspiracy, with title, content and url.\n",
    "First of all we need to import praw and access reddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import pandas as pd\n",
    "reddit = praw.Reddit(client_id = \"\", client_secret= \"\", user_agent= \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now access the subreddit, extract all the posts url in it and check which other subreddit reposted that posts and how much:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "post_list=list()\n",
    "subreddit_list = list()\n",
    "conspiracy_dict=dict()\n",
    "\n",
    "for i in reddit.subreddit(\"conspiracy\").top(limit=5000): #recupera i 5000 post top di conspiracy\n",
    "    post_list.append((i.title, i.score, i.url)) #recupera titolo, numero di upvote + il permalink\n",
    "\n",
    "\n",
    "for post in post_list:\n",
    "    for repost in reddit.subreddit('all').search('url:'+post[2]): #trova quando il post Ã¨ stato citato\n",
    "        subreddit_url = str(repost.subreddit)\n",
    "        subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "        if subreddit_url in conspiracy_dict.keys():\n",
    "            conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "            conspiracy_dict[subreddit_url][1][0] +=1\n",
    "        else:\n",
    "            conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "            conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "            \n",
    "                \n",
    "\n",
    "\n",
    "df = pd.DataFrame(conspiracy_dict)\n",
    "df.to_csv(r'results/conspiracy_data/conspiracy_top_url.csv',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the network\n",
    "Now we need to create the network of subreddits involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_in_dir(dir):\n",
    "    for filename in os.listdir(dir):\n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f) and f[-4:] == \".csv\":\n",
    "            yield f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "datasets= [file for file in get_all_in_dir(\"results/1st_level\")]\n",
    "datasets.extend(file for file in get_all_in_dir(\"results/2nd_level\"))\n",
    "\n",
    "network_list = list()\n",
    "\n",
    "for subr in datasets:\n",
    "        #print(f'now opening {subr}')\n",
    "        try:\n",
    "                df1 = pd.read_csv(subr)\n",
    "        except:\n",
    "                pass\n",
    "        try:\n",
    "                df1 = pd.read_csv(subr, encoding='utf8')\n",
    "        except:\n",
    "                print(f'unable to open {subr}')\n",
    "                continue\n",
    "        sub_name=subr[subr.find(\"\\\\\")+1:-4]\n",
    "        for column in df1.columns:\n",
    "                if column [:7] != \"Unnamed\":\n",
    "                        col_name = column[column.find(\"/r/\")+3:]\n",
    "                        try:\n",
    "                                network_list.append((sub_name, col_name, df1[column][1]))\n",
    "                        except Exception as E:\n",
    "                                print(E) \n",
    "                else: \n",
    "                        continue\n",
    "print(network_list)\n",
    "\n",
    "'''with open('Network.csv', 'w') as f:\n",
    "      \n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "      \n",
    "    write.writerow([\"Source\", \"Target\", \"Weight\"])\n",
    "    write.writerows(network_list)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for subnetworks in the original csvs\n",
    "Now that we have divided the main network in subnetworks we can llok for them in the original files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_in_dir(dir):\n",
    "    for filename in os.listdir(dir):\n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f) and f[-4:] == \".csv\":\n",
    "            yield f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "datasets= [file for file in get_all_in_dir(\"results/1st_level\")]\n",
    "datasets.extend(file for file in get_all_in_dir(\"results/2nd_level\"))\n",
    "\n",
    "networks=[file for file in get_all_in_dir(\"results/subnetworks/mixed\")]\n",
    "\n",
    "for net in networks:\n",
    "        try:\n",
    "                df_network = pd.read_csv(net)\n",
    "        except:\n",
    "                pass\n",
    "        try:\n",
    "                df_network = pd.read_csv(net, encoding='utf8')\n",
    "        except:\n",
    "                print(f'unable to open {subr}')\n",
    "                continue\n",
    "        network_name=net[net.find(\"mixed\\\\\")+6:-4]\n",
    "        network_name=network_name+\"_done\"\n",
    "        network_list=list()\n",
    "        for subr in datasets:\n",
    "                #print(f'now opening {subr}')\n",
    "                try:\n",
    "                        df_dataset = pd.read_csv(subr)\n",
    "                except:\n",
    "                        pass\n",
    "                try:\n",
    "                        df_dataset = pd.read_csv(subr, encoding='utf8')\n",
    "                except:\n",
    "                        print(f'unable to open {subr}')\n",
    "                        continue\n",
    "                sub_name=subr[subr.find(\"\\\\\")+1:-4]\n",
    "                for column in df_dataset.columns:\n",
    "                        if column [:7] != \"Unnamed\":\n",
    "                                col_name = column[column.find(\"/r/\")+3:]\n",
    "                                for index, row in df_network.iterrows():\n",
    "                                        if (col_name == row[\"Source\"] and sub_name == row[\"Target\"]) or (col_name == row[\"Target\"] and sub_name == row[\"Source\"]):\n",
    "                                                try:\n",
    "                                                        network_list.append((sub_name, col_name, df_dataset[column][1],df_dataset[column][0]))\n",
    "                                                except Exception as E:\n",
    "                                                        print(E) \n",
    "                                        else: \n",
    "                                                continue\n",
    "        \n",
    "        \n",
    "        try:\n",
    "                with open(network_name+'.csv', 'w', encoding='utf-8') as f:\n",
    "        # using csv.writer method from CSV package\n",
    "                        write = csv.writer(f)\n",
    "                        write.writerow([\"Source\", \"Target\", \"Weight\",\"Comments\"])\n",
    "                        write.writerows(network_list)\n",
    "        except Exception as E:\n",
    "                print(E)\n",
    "\n",
    "                \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look for each subnetwork most connected subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n most connected subreddits\n",
    "def takeSecond(elem):\n",
    "    return int(elem[2])\n",
    "\n",
    "sorted(network_list, key=takeSecond, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_in_dir(dir):\n",
    "    for filename in os.listdir(dir):\n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f) and f[-4:] == \".csv\":\n",
    "            yield f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def iterSample(iterable, samplesize):\n",
    "    results = []\n",
    "    for i, v in enumerate(iterable):\n",
    "        r = random.randint(0, i)\n",
    "        if r < samplesize:\n",
    "            if i < samplesize:\n",
    "                results.insert(r, v) # add first samplesize items in random order\n",
    "            else:\n",
    "                results[r] = v # at a decreasing rate, replace random items\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the dataset and the comments from the posts in common between subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "to_scan= [file for file in get_all_in_dir(\"results/subnetworks/mixed\")]\n",
    "top_comments_list = dict()\n",
    "for subreddit in to_scan:\n",
    "        try: \n",
    "                df = pd.read_csv(subreddit, sep=',', encoding='utf8', on_bad_lines='skip').dropna()\n",
    "                if os.path.isfile('comments_'+subreddit.split('\\\\')[1]):\n",
    "                        start = len(pd.read_csv('comments_'+subreddit.split('\\\\')[1]))\n",
    "                        df= df[(df.index == start).idxmin():]\n",
    "        except:\n",
    "                print('error',subreddit)\n",
    "                continue\n",
    "        for row in df.to_dict(orient='records'):\n",
    "                try:\n",
    "                        list_post=[]\n",
    "                        list_post= row['Posts'][1:-1].split(',')\n",
    "                except Exception as e:\n",
    "                        continue\n",
    "                result = ''\n",
    "                num = 0\n",
    "                for post_url in list_post:\n",
    "                        if post_url == 0:\n",
    "                                continue\n",
    "                        post_url = post_url.replace(' ','')\n",
    "                        #tmp =  re.sub(\"\\[\\]\\'\", \"\", post_url) NON FUNZIONA E NON CAPISCO PERCHÃ©\n",
    "                        try:\n",
    "                                post = reddit.submission(url=post_url[1:-1])\n",
    "                                try:\n",
    "                                        #This allows up to 1 reply to each post.\n",
    "                                        post.comments.replace_more(limit=1)\n",
    "                                except Exception as e:\n",
    "                                        print(e)\n",
    "                                        print(f'{post}s comments was not accessible')\n",
    "                                        continue\n",
    "                                num += len(post.comments)\n",
    "                                if num > 500:\n",
    "                                        for comment in iterSample(post.comments, 500):\n",
    "                                                try:\n",
    "                                                        result += comment.body\n",
    "                                                        for reply in comment.replies:\n",
    "                                                                result+= reply.body\n",
    "                                                        result=result.replace(',','')\n",
    "                                                        result=result.replace('\\n','\\s')\n",
    "                                                except:\n",
    "                                                        continue\n",
    "                                else:\n",
    "                                        for comment in post.comments:\n",
    "                                                try:\n",
    "                                                        result += comment.body\n",
    "                                                        for reply in comment.replies:\n",
    "                                                                result+= reply.body\n",
    "                                                        result=result.replace(',','')\n",
    "                                                        result=result.replace('\\n','\\s')\n",
    "                                                except:\n",
    "                                                        continue\n",
    "                        except Exception as e:\n",
    "                                print(e)\n",
    "                                print(post.permalink + ' is not accessible')\n",
    "                                continue\n",
    "                                        \n",
    "                res=pd.DataFrame.from_dict([{'Source':row['Source'], 'Target':row['Target'],'NumberComments': num,'Comments':result}])\n",
    "                if os.path.isfile('comments_'+subreddit.split('\\\\')[1]):\n",
    "                        res.to_csv('comments_'+subreddit.split('\\\\')[1],  encoding='utf8', mode='a', header=False)\n",
    "                else:\n",
    "                        res.to_csv('comments_'+subreddit.split('\\\\')[1],  encoding='utf8')\n",
    "                        \n",
    "        print(subreddit, ' is finished')\n",
    "                        \n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69eb92836b941e979072a76c7fcfffe5419cca933cedd02cfafbdfca1a93358c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
