{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Scraping\n",
    "In order to scrape posts from Reddit, we used the [`praw` python library](https://github.com/praw-dev/praw). \n",
    "In order to use that, we need:\n",
    "- a Reddit account.\n",
    "- a Reddit app ([here](https://www.reddit.com/prefs/apps) you can create one). Here you'll find the client id under the app name once you create it, as well as the client secret and the user agent, which is your app name.\n",
    "\n",
    "Once we have those, we can access reddit from the terminal using these steps(it is also possible to access as a user by adding username and account if you want to use it in write mode):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install praw\n",
    "import praw\n",
    "reddit = praw.Reddit(client_id='', client_secret='', user_agent='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyse posts, post requirements etc. for a single subreddit or for all of them(by using the subreddit 'all'); we can also select top posts or do searches. \n",
    "# sistemare data utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’ve found a few funny memories during lockdown. This is from my 1st tour in 89, backstage in Vegas. Author: ReallyRickAstley Link: https://www.reddit.com/r/pics/comments/haucpf/ive_found_a_few_funny_memories_during_lockdown/ Subreddit: r/pics Upvote Ratio: 0.99 Date: 1592410647.0\n",
      "Times Square right now Author: SomeGuyInDeutschland Link: https://www.reddit.com/r/wallstreetbets/comments/l8rf4k/times_square_right_now/ Subreddit: r/wallstreetbets Upvote Ratio: 0.99 Date: 1612029638.0\n",
      "Joe Biden elected president of the United States Author: throwawaynumber53 Link: https://www.reddit.com/r/news/comments/jptqj9/joe_biden_elected_president_of_the_united_states/ Subreddit: r/news Upvote Ratio: 0.88 Date: 1604766517.0\n",
      "The Senate. Upvote this so that people see it when they Google \"The Senate\". Author: serventofgaben Link: https://www.reddit.com/r/movies/comments/62sjuh/the_senate_upvote_this_so_that_people_see_it_when/ Subreddit: r/movies Upvote Ratio: 0.96 Date: 1491051474.0\n",
      "My cab driver tonight was so excited to share with me that he’d made the cover of the calendar. I told him I’d help let the world see Author: the_Diva Link: https://www.reddit.com/r/funny/comments/7mjw12/my_cab_driver_tonight_was_so_excited_to_share/ Subreddit: r/funny Upvote Ratio: 0.97 Date: 1514430055.0\n"
     ]
    }
   ],
   "source": [
    "for i in reddit.subreddit('all').top(limit=5): #this will print the top posts all time\n",
    "    print(i.title + ' Author: ' + i.author.name +  ' Link: https://www.reddit.com' + i.permalink + ' Subreddit: r/' + i.subreddit.display_name + ' Upvote Ratio: ' + str(i.upvote_ratio) + ' Date: '+str(i.created_utc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserire spiegazione differenza tra le diverse scale hot top controversial\n",
    "Now, for our analysis we will use data from the subreddit r/conspiracy and we will try to study the ramifications of the posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\npost_list=list()\\nsubreddit_list = list()\\nconspiracy_dict=dict()\\n\\n#put the subreddit already passed here\\nid_to_avoid = [reddit.subreddit(\\'conspiracy\\').id]\\n\\nfor i in reddit.subreddit(\"conspiracy\").hot(limit=5): #recupera i 5000 post hot di conspiracy\\n    post_list.append((i.title, i.score, i.url)) #recupera titolo, numero di upvote + l\\'url che viene postato\\n\\n\\nfor post in post_list:\\n    for repost in reddit.subreddit(\\'all\\').search(\\'url:\\'+post[2]): #trova quando il post è stato citato\\n        if repost.subreddit_id not in id_to_avoid:\\n            \\n            subreddit_url = str(repost.subreddit)\\n            subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\\n            if subreddit_url in conspiracy_dict.keys():\\n                conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\\n                conspiracy_dict[subreddit_url][1][0] +=1\\n            else:\\n                conspiracy_dict[subreddit_url]=[[],[1]]\\n                conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\\n                \\n\\n\\ndf = pd.DataFrame(conspiracy_dict)\\ndf.to_csv(r\\'results/conspiracy_hot_url.csv\\',index=False)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "post_list=list()\n",
    "subreddit_list = list()\n",
    "conspiracy_dict=dict()\n",
    "\n",
    "#put the subreddit already passed here\n",
    "id_to_avoid = [reddit.subreddit('conspiracy').id]\n",
    "\n",
    "for i in reddit.subreddit(\"conspiracy\").hot(limit=5): #recupera i 5000 post hot di conspiracy\n",
    "    post_list.append((i.title, i.score, i.url)) #recupera titolo, numero di upvote + l'url che viene postato\n",
    "\n",
    "\n",
    "for post in post_list:\n",
    "    for repost in reddit.subreddit('all').search('url:'+post[2]): #trova quando il post è stato citato\n",
    "        if repost.subreddit_id not in id_to_avoid:\n",
    "            \n",
    "            subreddit_url = str(repost.subreddit)\n",
    "            subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "            if subreddit_url in conspiracy_dict.keys():\n",
    "                conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                conspiracy_dict[subreddit_url][1][0] +=1\n",
    "            else:\n",
    "                conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "                conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                \n",
    "\n",
    "\n",
    "df = pd.DataFrame(conspiracy_dict)\n",
    "df.to_csv(r'results/conspiracy_hot_url.csv',index=False)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting thing we noted is that a good part of the propagation of news comes from bots, especially from the top posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDa fare: script che accumula tutti gli id nella cartella\\nLet\\'s suppose that the results are in \"results/conspiracy_hot.csv\". \\nIf you have multiple csv in the directory \"my_directory\", you may uncomment this and use the for loop in here:\\nimport os\\ndir = \"my_directory\"\\nto_scan = [] #iterate over this\\nfor filename in os.listdir(directory):\\n    f = os.path.join(directory, filename)\\n    if os.path.isfile(f) and f[-4:] == \".csv\":\\n        to_scan.append(f) \\nid_to_analyse = []\\n\\nfor file in to_scan:\\n    df1 = pd.read_csv(\\'results/conspiracy_hot.csv\\')\\n    for column in df1.columns:\\n        value = df1[column][1][1:-1]\\n        if int(value) >= 5:\\n            ind = column.index(\\'/r/\\')\\n            id = column[ind+3:]\\n            id_to_analyse.append(id)\\n\\n\\ndf1 = pd.read_csv(\\'results/conspiracy_hot.csv\\')\\nid_to_analyse = []\\nfor column in df1.columns:\\n    value = df1[column][1][1:-1]\\n    if int(value) >= 5:\\n        ind = column.index(\\'/r/\\')\\n        id = column[ind+3:]\\n        id_to_analyse.append(id)\\n\\nfor subreddit in id_to_analyse:\\n    post_list=list()\\n    subreddit_list = list()\\n    conspiracy_dict=dict() \\n    for i in reddit.subreddit(subreddit).top(limit=500):\\n        post_list.append((i.title, i.score, \"https://www.reddit.com\"+i.permalink))\\n\\n\\n    for post in post_list:\\n        for repost in reddit.subreddit(\\'all\\').search(\\'url:\\'+post[2]):\\n            if repost.subreddit_id != subreddit: #cosa facciamo?\\n                subreddit_url = str(repost.subreddit)\\n                subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\\n                if subreddit_url in conspiracy_dict.keys():\\n                    conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\\n                    conspiracy_dict[subreddit_url][1][0] +=1\\n                else:\\n                    conspiracy_dict[subreddit_url]=[[],[1]]\\n                    conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\\n    df = pd.DataFrame(conspiracy_dict)\\n    df.to_csv(r\\'results/\\'+subreddit+\\'.csv\\')\\n    '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Da fare: script che accumula tutti gli id nella cartella\n",
    "Let's suppose that the results are in \"results/conspiracy_hot.csv\". \n",
    "If you have multiple csv in the directory \"my_directory\", you may uncomment this and use the for loop in here:\n",
    "import os\n",
    "dir = \"my_directory\"\n",
    "to_scan = [] #iterate over this\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if os.path.isfile(f) and f[-4:] == \".csv\":\n",
    "        to_scan.append(f) \n",
    "id_to_analyse = []\n",
    "\n",
    "for file in to_scan:\n",
    "    df1 = pd.read_csv('results/conspiracy_hot.csv')\n",
    "    for column in df1.columns:\n",
    "        value = df1[column][1][1:-1]\n",
    "        if int(value) >= 5:\n",
    "            ind = column.index('/r/')\n",
    "            id = column[ind+3:]\n",
    "            id_to_analyse.append(id)\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('results/conspiracy_hot.csv')\n",
    "id_to_analyse = []\n",
    "for column in df1.columns:\n",
    "    value = df1[column][1][1:-1]\n",
    "    if int(value) >= 5:\n",
    "        ind = column.index('/r/')\n",
    "        id = column[ind+3:]\n",
    "        id_to_analyse.append(id)\n",
    "\n",
    "for subreddit in id_to_analyse:\n",
    "    post_list=list()\n",
    "    subreddit_list = list()\n",
    "    conspiracy_dict=dict() \n",
    "    for i in reddit.subreddit(subreddit).top(limit=500):\n",
    "        post_list.append((i.title, i.score, \"https://www.reddit.com\"+i.permalink))\n",
    "\n",
    "\n",
    "    for post in post_list:\n",
    "        for repost in reddit.subreddit('all').search('url:'+post[2]):\n",
    "            if repost.subreddit_id != subreddit: #cosa facciamo?\n",
    "                subreddit_url = str(repost.subreddit)\n",
    "                subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "                if subreddit_url in conspiracy_dict.keys():\n",
    "                    conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                    conspiracy_dict[subreddit_url][1][0] +=1\n",
    "                else:\n",
    "                    conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "                    conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "    df = pd.DataFrame(conspiracy_dict)\n",
    "    df.to_csv(r'results/'+subreddit+'.csv')\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "To do sentiment analysis, we will use the syuzhet package in R.\n",
    "First we need to extract the comments inside a Reddit post. We will ignore the comments of bots, since they do not provide information about how the user interacts with it, even though it can interfere with it.\n",
    "https://realpython.com/python-nltk-sentiment-analysis/\n",
    "Caveat: \\s indica il sarcasmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def iterSample(iterable, samplesize):\n",
    "    results = []\n",
    "    for i, v in enumerate(iterable):\n",
    "        r = random.randint(0, i)\n",
    "        if r < samplesize:\n",
    "            if i < samplesize:\n",
    "                results.insert(r, v) # add first samplesize items in random order\n",
    "            else:\n",
    "                results[r] = v # at a decreasing rate, replace random items\n",
    "\n",
    "    if len(results) < samplesize:\n",
    "        raise ValueError(\"Sample larger than population.\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/esist/comments/7hh4z8/i_was_just_handed_a_479page_tax_bill_a_few_hours/ was not accessible\n",
      "https://www.reddit.com/r/AssholesGetBanned/comments/mqbfns/damn_this_one_was_too_easy/ was not accessible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_13228/1822408351.py\", line 13, in <module>\n",
      "    df = pd.read_csv(subreddit, sep=';', encoding='unicode_escape')\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 586, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 488, in _read\n",
      "    return parser.read(nrows)\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1047, in read\n",
      "    index, columns, col_dict = self._engine.read(nrows)\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\", line 224, in read\n",
      "    chunks = self._reader.read_low_memory(nrows)\n",
      "  File \"pandas\\_libs\\parsers.pyx\", line 801, in pandas._libs.parsers.TextReader.read_low_memory\n",
      "  File \"pandas\\_libs\\parsers.pyx\", line 857, in pandas._libs.parsers.TextReader._read_rows\n",
      "  File \"pandas\\_libs\\parsers.pyx\", line 843, in pandas._libs.parsers.TextReader._tokenize_rows\n",
      "  File \"pandas\\_libs\\parsers.pyx\", line 1925, in pandas._libs.parsers.raise_parser_error\n",
      "pandas.errors.ParserError: Error tokenizing data. C error: Expected 1 fields in line 7, saw 942\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/conspiracy_url_copy.csv should be opened with different settings.\n",
      "results/WayOfTheBern.csv has problems in formatting.\n",
      "results/WhitePeopleTwitter.csv has problems in formatting.\n",
      "results/YangForPresidentHQ.csv has problems in formatting.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import traceback\n",
    "top_comments_list = []\n",
    "import os\n",
    "dir = \"results/\"\n",
    "to_scan = [] #iterate over this\n",
    "for filename in os.listdir(dir):\n",
    "    f = os.path.join(dir, filename)\n",
    "    if os.path.isfile(f) and f[-4:] == \".csv\":\n",
    "        to_scan.append(f) \n",
    "for subreddit in to_scan:\n",
    "        try:\n",
    "                df = pd.read_csv(subreddit, sep=';', encoding='unicode_escape')\n",
    "        except:\n",
    "                traceback.print_exc()\n",
    "                print(f'{subreddit} should be opened with different settings.')\n",
    "                continue\n",
    "        for column in df.columns: # qui inserire un loop sui permalink\n",
    "                try:\n",
    "                        list_post= df[column][1].split(',')\n",
    "                except:\n",
    "                        print(f'{subreddit} has problems in formatting.')\n",
    "                        pass\n",
    "                for post_url in list_post:\n",
    "                        tmp = post_url\n",
    "                        \n",
    "                        while \"'\" in tmp:\n",
    "                                tmp = tmp.replace(\"'\",\"\")\n",
    "                        while \"[\" in tmp:\n",
    "                                tmp = tmp.replace(\"[\",\"\")\n",
    "                        while \"]\" in tmp:\n",
    "                                tmp = tmp.replace(']','')\n",
    "                        try:\n",
    "                                post = reddit.submission(url=tmp)\n",
    "                        except: # some posts may be removed\n",
    "                                continue\n",
    "                        try:\n",
    "                                post.comments.replace_more(limit=0)\n",
    "                        except:\n",
    "                                print(f'{tmp} was not accessible')\n",
    "                                continue\n",
    "                        result = []\n",
    "                        if len(post.comments)> 500:\n",
    "                                for comment in iterSample(post.comments, 500):\n",
    "                                        result.append(comment)\n",
    "                        else:\n",
    "                                for comment in post.comments:\n",
    "                                        result.append(comment)\n",
    "                        result={'permalink':tmp,'comments': result, 'target': df[column][0]}\n",
    "                        top_comments_list.append(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['stopwords', \"vader_lexicon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from statistics import mean\n",
    "sent = SentimentIntensityAnalyzer()\n",
    "sentiment_values = list()\n",
    "for post in top_comments_list:\n",
    "    post_sentiment = [] # median sentiment\n",
    "    bot = \"I'm a bot\"\n",
    "    \n",
    "    for comment in post['comments']:\n",
    "        body = comment.body\n",
    "        if not \"I'm a bot\" in body and not 'I am a bot' in body:\n",
    "            for reply in comment.replies:\n",
    "                if not \"I'm a bot\" in body and not 'I am a bot' in reply.body:\n",
    "                    body += reply.body\n",
    "                for second_reply in reply.replies:\n",
    "                    if not bot in second_reply.body:\n",
    "                        body += second_reply.body\n",
    "            val = sent.polarity_scores(body)['compound']\n",
    "            post_sentiment.append(val)\n",
    "    try:\n",
    "        post_sentiment = mean(post_sentiment)\n",
    "    except:\n",
    "        post_sentiment = 0\n",
    "    sentiment_values.append({'link': post['permalink'],'sentiment': post_sentiment, 'origin': reddit.submission('url:'+post['permalink']),'posted_in': post['target']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.DataFrame(sentiment_values)\n",
    "sentiment_df.to_csv('results/sentiment/sentiment.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d65f8af9b1347936d5c0a715a1a101b7602968bee42a1bc2161adfc924f1cbb0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
