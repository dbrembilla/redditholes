{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RedditHoles - A study in  internet Rabbit Holes\n",
    "\n",
    "## An introduction to Reddit\n",
    "\n",
    "Reddit is a social media platform organised in communities, rather than individual connections. This makes the experience on the platform quite different from other Social Networks and closer, in a way, to the one of forums.  \n",
    "\n",
    "### u/User\n",
    "A user (also called redditor, usually referred as “u/” followed by the username) can make a post (also called a submission); posts (also called submissions) can be links, videos, pictures, polls or text. The OP (Original Poster) as well as other users can comment and vote (either upvote or downvote) the post if they find it interesting (so it’s not exactly like like/dislike features of other Social Networks). Finally, users can give posts awards by paying for reddit coins; these recognise the contributions of a post or comment. There are hundreds of these, from a generic gold or silver award to some following internet lingo, such as the “F” award (used ironically to ‘pay respect’). \n",
    "\n",
    "Users can mark their posts as spoilers and multiple types of flairs and markings such as OC (Original Content), Spoilers, +18 and so on. Some subreddits might have rules for posting. \n",
    "\n",
    "Users can be humans or bots. Bots have multiple uses, from auto moderation, to answering with quotes from movies and books, waving flags to other utilities and fun uses. \n",
    "#### The average redditor\n",
    "\n",
    "Reddit users, according to [data from the site itself](https://www.redditinc.com/advertising/audience), are more than 50 million with more than 100 thousand communities. They are mostly male (56%) and have between 18 and 34 year old (58%). \n",
    "\n",
    "### r/Subreddits\n",
    "\n",
    "Reddit is structured in subreddits, communities that group in various ways the interested users. Subreddits can vary from cute pets, to political parties, to recipe advice. Subreddits are usually referred to as “r/” followed by the name of the subreddits (in our case, we will study “r/conspiracy” and related subreddits). \n",
    "\n",
    "Subreddits can vary significantly. They can have rules for posting, different levels of moderation and bot acceptance, all depending from the nature of the subreddit. Rules of subreddits can have different nature; some subreddits may require the content of a post to be marked, some require sources to be linked, some can have no rules at all. This ambiguity makes Reddit a much more decentralised and open Social Network in which some of the cut down on fake news and trolls which happened on sites such as Facebook or Twitter has not yet happened at the same level. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Reddit Data\n",
    "In order to access data from the Reddit API, we used the [`praw` python library](https://github.com/praw-dev/praw). \n",
    "In order to use that, we need:\n",
    "- a Reddit account.\n",
    "- a Reddit app ([here](https://www.reddit.com/prefs/apps) you can create one). Here you'll find the client id under the app name once you create it, as well as the client secret and the user agent, which is your app name.\n",
    "\n",
    "Once we have those, we can access reddit from the python terminal by creating a reddit instance in praw (it is also possible to access as a user by adding username and account if you want to use it in write mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install praw\n",
    "#!pip install pandas\n",
    "import praw\n",
    "reddit = praw.Reddit(client_id = \"CMO4Nf8Dpd3YE_lftmqnHg\", client_secret= \"79YxaeUzQYqtVpvev6SbWbCMvF70-g\", user_agent= \"little_digging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can access information about subreddits, posts and users. As an example, we can access the top 5 posts of all time on Reddit, with the information about the author and the upvote ratio and datat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reddit.subreddit('all').top(limit=5): #this will print the top posts all time\n",
    "    print(i.title + ' Author: ' + i.author.name +  ' Link: https://www.reddit.com' + i.permalink + ' Subreddit: r/' + i.subreddit.display_name + ' Upvote Ratio: ' + str(i.upvote_ratio) + ' Date (UTC Format): '+str(i.created_utc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Reddit, starting from r/conspiracy\n",
    "\n",
    "Our objective is to watch how a piece of news or a post is shared between different subreddits. While most social network would measure shares of a post, Reddit is built in a way that if a link is shared in the platform, it is possible to retrieve how much the original link is shared through subreddits. As an example, if an image from imgur is shared, we can use its url to search for the same object on Reddit.\n",
    "This happens because, for the most part, Reddit is used to comment news and multimedia in communities, which often present a common worldview (e.g. subbreddits made by people with same political views).\n",
    "We started with the [r/conspiracy](https://www.reddit.com/r/conspiracy/)'s posts, in particular the top 5000 posts all time. We opted for the top posts all time because the other types of ranking are time-bound and we wanted to watch the overall transmission of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "post_list=list()\n",
    "subreddit_list = list()\n",
    "conspiracy_dict=dict()\n",
    "\n",
    "for i in reddit.subreddit(\"conspiracy\").top(limit=5000): \n",
    "    post_list.append(i.url) # This may seem counterintuitive, but in praw's terms this is the original link of the resource inside the post.\n",
    "\n",
    "\n",
    "for post in post_list:\n",
    "    for repost in reddit.subreddit('all').search('url:'+post): # This function searches for the original post's element\n",
    "        subreddit_url = str(repost.subreddit)\n",
    "        subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "        if subreddit_url in conspiracy_dict.keys():\n",
    "            conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "            conspiracy_dict[subreddit_url][1][0] +=1\n",
    "        else:\n",
    "            conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "            conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "            \n",
    "                \n",
    "\n",
    "\n",
    "df = pd.DataFrame(conspiracy_dict)\n",
    "df.to_csv(r'results/conspiracy_data/conspiracy.csv',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realised after the fact that this method also got the reposts inside the same subreddit. After manually cleaning the csv in this instance, we proceeded to remove this problem in the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a network of subreddits\n",
    "\n",
    "After scraping r/conspiracy, we moved to the neighbouring subreddits. What this will do is creating a network of shared posts between subreddits.\n",
    "\n",
    "First we need to look at all the files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_in_dir(dir):\n",
    "    for filename in os.listdir(dir):\n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f) and f[-4:] == \".csv\":\n",
    "            yield f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can scrape the other subreddits that had more than 5 posts in common with r/conspiracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/conspiracy_data/conspiracy.csv')\n",
    "id_to_analyse = []\n",
    "for column in df1.columns:\n",
    "    value = df1[column][1][1:-1]\n",
    "    if int(value) >= 5:\n",
    "        ind = column.index('/r/')\n",
    "        id = column[ind+3:]\n",
    "        id_to_analyse.append(id)\n",
    "        \n",
    "\n",
    "for subreddit in id_to_analyse:\n",
    "    post_list=list()\n",
    "    subreddit_list = list()\n",
    "    conspiracy_dict=dict() \n",
    "    for i in reddit.subreddit(subreddit).top(limit=5000):\n",
    "        post_list.append((i.title, i.score, i.url))\n",
    "\n",
    "\n",
    "\n",
    "    for post in post_list:\n",
    "        for repost in reddit.subreddit('all').search('url:'+post[2]):\n",
    "            if repost.subreddit_id != \"t5_\"+reddit.subreddit(subreddit).id: #cosa facciamo?\n",
    "                subreddit_url = str(repost.subreddit)\n",
    "                subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "                if subreddit_url in conspiracy_dict.keys():\n",
    "                    conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                    conspiracy_dict[subreddit_url][1][0] +=1\n",
    "                else:\n",
    "                    conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "                    conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "    df = pd.DataFrame(conspiracy_dict)\n",
    "    df.to_csv(r'results/1st_level/'+subreddit+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the cycle one more time, but this time with just the top 500 posts, in order to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "top_comments_list = dict()\n",
    "to_scan = [file for file in get_all_in_dir(\"results/1st_level_2\")]\n",
    "to_avoid= [file.split('\\\\')[1] for file in get_all_in_dir(\"results/1st_level\")]\n",
    "to_avoid.extend(file.split('\\\\')[1] for file in get_all_in_dir(\"results/1st_level_2\"))\n",
    "to_avoid.extend(file.split('\\\\')[1] for file in get_all_in_dir(\"results/2nd_level\"))\n",
    "\n",
    "for subr in to_scan:\n",
    "        id_to_analyse = []\n",
    "        print(f'now opening {subr}')\n",
    "        try:\n",
    "                df1 = pd.read_csv(subr)\n",
    "        except:\n",
    "                pass\n",
    "        try:\n",
    "                df1 = pd.read_csv(subr, encoding='utf8')\n",
    "        except:\n",
    "                print(f'unable to open {subr}')\n",
    "                continue\n",
    "        for column in df1.columns:\n",
    "                try: \n",
    "                        value = df1[column][1][1:-1]\n",
    "                except:\n",
    "                        continue\n",
    "                if int(value) >= 5:\n",
    "                        ind = column.index('/r/')\n",
    "                        id = column[ind+3:]\n",
    "                        if not id + '.csv' in to_avoid:\n",
    "                                try:\n",
    "                                        post_list=list()\n",
    "                                        subreddit_list = list()\n",
    "                                        conspiracy_dict=dict() \n",
    "                                        for i in reddit.subreddit(id).top(limit=500):\n",
    "                                                post_list.append((i.title, i.score, i.url))\n",
    "\n",
    "                                        for post in post_list:\n",
    "                                                for repost in reddit.subreddit('all').search('url:'+post[2]):\n",
    "                                                        if repost.subreddit_id != \"t5_\"+reddit.subreddit(id).id: #cosa facciamo?\n",
    "                                                                subreddit_url = str(repost.subreddit)\n",
    "                                                                subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "                                                        else:\n",
    "                                                                continue\n",
    "                                                        if subreddit_url in conspiracy_dict.keys():\n",
    "                                                                conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                                                                conspiracy_dict[subreddit_url][1][0] +=1\n",
    "                                                        else:\n",
    "                                                                conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "                                                                conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                                        \n",
    "                                        df = pd.DataFrame(conspiracy_dict)\n",
    "                                        df.to_csv(r'results/2nd_level/'+id+'.csv')\n",
    "                                        with open(\"results/2nd_level/done_2.txt\",'a', encoding = \"utf-8\") as text_note:\n",
    "                                                text_note.write(id + \"\\n\")\n",
    "                                                text_note.close()\n",
    "                                        \n",
    "                                except Exception as E:\n",
    "                                        print(E)\n",
    "                                        with open(\"results/2nd_level/error_2.txt\",'a', encoding = \"utf-8\") as text_note:\n",
    "                                                text_note.write(id + \"\\n\")\n",
    "                                                text_note.close()\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "In order to perform more efficiently the operations of data representation we decided two perform two operations:\n",
    "1. We turned the number of crossposts into integers.\n",
    "2. We removed subreddits with less than 5 posts in common.\n",
    "These was in order to have cleaner and more usable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing parentheses and connections with less than 5 reposts\n",
    "import pandas as pd\n",
    "datasets= [file for file in get_all_in_dir(\"results/1st_level\")]\n",
    "datasets.extend(file for file in get_all_in_dir(\"results/1st_level_2\"))\n",
    "datasets.extend(file for file in get_all_in_dir(\"results/2nd_level\"))\n",
    "datasets.extend(\"results/conspiracy_data/conspiracy_top_url.csv\")\n",
    "\n",
    "for file in datasets:\n",
    "    try:\n",
    "        df = pd.read_csv(file, sep=',', on_bad_lines='skip', encoding='utf8')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        df = pd.read_csv(file, on_bad_lines='skip', encoding='latin')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(file, 'has problems in formatting')\n",
    "        continue\n",
    "    for col in df.columns:\n",
    "        if \"u/\" in col or 'u_' in col: #Sometimes users end up in the columns. Remove them. \n",
    "            df.drop(col, inplace=True, axis=1)\n",
    "        else:\n",
    "            try:\n",
    "                \n",
    "               if isinstance(df[col][1], str): \n",
    "                    try:\n",
    "                        df[col][1]=int(df[col][1][1:-1])\n",
    "                    except:\n",
    "                        continue\n",
    "                    if df[col][1]<5:\n",
    "                        df.drop(col, inplace=True, axis=1)\n",
    "            except:\n",
    "                df.drop(col, inplace=True, axis=1)\n",
    "    df.to_csv(file, encoding=\"utf8\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data from Reddit, we can build a network of shared posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Network of crossposts\n",
    "By using Gephi, we plotted the network of crosspost. By doing this, we found some communities of subreddits that present a significant number of crossposts.\n",
    "1. <b>Crypto, Tech and other news</b>, with a part dedicated to tech (BitcoinAll, Mistifront, Libertarian, Technews and brprogramming) and another about politics and conspiracies (conspiracy, politics...).\n",
    "2. <b>MetaReddit</b>, subreddits that share a meta-reddit approach (e.g. bots collecting stories from everywhere or comments about reddit), that are also very interlinked with generalist subreddits (such as r/pics). Among these are mistyfront, SubredditNN, Blackout2015. This is very intelinked with tech and general politics.  \n",
    "3. <b>Politics/News</b> with various news and political subreddits, from either party or with no party affiliation, but also with a significant conspiracy news subgroups. Among these are anythingGoesNews, ConspiracyII, Coronavirus, infrasociology\n",
    "4. <b>Leftist Politics</b> centred around WayOfTheBern, LateStageCapitalism, SocialismAndVeganism, but also TrueReddit.\n",
    "5. <b>Bernie Sanders</b> - Started from StillSandersForPresident, WayOfTheBern, RealBlueMidterm (central to other minor subreddits) A significant group of subreddits all sharing the topic of popularising the figure of Bernie Sanders. The main gateways seem to be NEwYorkSanders,FLoridaforSanders ad CaliforniaFOrSanders.\n",
    "6. <b>Conservative/Libertarian</b> subreddits from the right and alt-right, with Conservative, Libertarian, Descent into tyranny but also  news sources such as Worldpolitics and alternative news subreddits such as Censorship \n",
    "7. <b>Science and Pseudoscience</b>, subreddits discussing both scientific news(environment, AutoNewspaper), views of the future(futurology) but also pseudoscience (ScienceUncensored, DebateVaccines)\n",
    "8. <b>Information</b> a subnetwork connected both to general information subreddits (todayilearned) and classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the network\n",
    "datasets= [file for file in get_all_in_dir(\"results/1st_level\")]\n",
    "datasets.extend(file for file in get_all_in_dir(\"results/1st_level_2\"))\n",
    "datasets.extend(file for file in get_all_in_dir(\"results/2nd_level\"))\n",
    "\n",
    "network_list = list()\n",
    "\n",
    "for subr in datasets:\n",
    "        #print(f'now opening {subr}')\n",
    "        try:\n",
    "                df1 = pd.read_csv(subr)\n",
    "        except:\n",
    "                pass\n",
    "        try:\n",
    "                df1 = pd.read_csv(subr, encoding='utf8')\n",
    "        except:\n",
    "                print(f'unable to open {subr}')\n",
    "                continue\n",
    "        sub_name=subr[subr.find(\"\\\\\")+1:-4]\n",
    "        for column in df1.columns:\n",
    "                try: \n",
    "                        value = column\n",
    "                except:\n",
    "                        continue\n",
    "                if column [:7] != \"Unnamed\":\n",
    "                        col_name = column[column.find(\"/r/\")+3:]\n",
    "                        try:\n",
    "                                network_list.append((sub_name, col_name, df1[column][1]))\n",
    "                        except Exception as E:\n",
    "                                print(E) \n",
    "                else: \n",
    "                        continue\n",
    "print(network_list)\n",
    "\n",
    "'''' with open('Network.csv', 'w') as f:\n",
    "      \n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "      \n",
    "    write.writerow([\"Source\", \"Target\", \"Weight\"])\n",
    "    write.writerows(network_list)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUI DOBBIAMO RIVEDERE L'ORGANIZZAZIONE; deve esserci un csv che contiene i post in comune tra csv1 e csv2, dicendo da dove arrivano. Questo ci permetterà di confrontare i risultanti.\n",
    "## Analysing the comments\n",
    "A part of our inquiry involves the kind of language that redditors use on the website in response to posts. In order to this, we employed two techniques that allow us to find out the nature of a text: Topic Modelling and Sentiment Analysis\n",
    "\n",
    "### Topic modelling\n",
    "Topic modelling is a machine learning technique that tries to predict the distribution of abstract topics in a text, and thus reveal the hidden semantic structures within it. In particular, we used Latent Dirichlet Allocation (LDA) method. We adapted the method used in [here](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#1introduction).\n",
    "The libraries used are <code>[nltk](https://www.nltk.org/), [gensim](https://github.com/RaRe-Technologies/gensim/), [spacy](https://spacy.io/), [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/readme.html)</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#!pip install gensim\n",
    "#!pip install spacy\n",
    "#!pip install pyLDAvis\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['re', 'edit']) \n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as genmodels  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "#python -m spacy download en_core_web_sm\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of comments can be particularly big, we used this code snippet to get a random sample from bigger comment sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def iterSample(iterable, samplesize):\n",
    "    results = []\n",
    "    for i, v in enumerate(iterable):\n",
    "        r = random.randint(0, i)\n",
    "        if r < samplesize:\n",
    "            if i < samplesize:\n",
    "                results.insert(r, v) # add first samplesize items in random order\n",
    "            else:\n",
    "                results[r] = v # at a decreasing rate, replace random items\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the dataset and the comments from the posts in common between subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening results/subnetworks\\bernie.csv\n",
      "results/subnetworks\\bernie.csv  is finished\n",
      "Opening results/subnetworks\\bitcoins.csv\n",
      "results/subnetworks\\bitcoins.csv  is finished\n",
      "Opening results/subnetworks\\conspiracies.csv\n",
      "results/subnetworks\\conspiracies.csv  is finished\n",
      "Opening results/subnetworks\\left.csv\n",
      "results/subnetworks\\left.csv  is finished\n",
      "Opening results/subnetworks\\meta_reddit.csv\n",
      "results/subnetworks\\meta_reddit.csv  is finished\n",
      "Opening results/subnetworks\\Network_1_bernie.csv\n",
      "results/subnetworks\\Network_1_bernie.csv  is finished\n",
      "Opening results/subnetworks\\Network_1_bitcoins.csv\n",
      "results/subnetworks\\Network_1_bitcoins.csv  is finished\n",
      "Opening results/subnetworks\\Network_1_conspiracies.csv\n",
      "results/subnetworks\\Network_1_conspiracies.csv  is finished\n",
      "Opening results/subnetworks\\Network_1_left.csv\n",
      "results/subnetworks\\Network_1_left.csv  is finished\n",
      "Opening results/subnetworks\\Network_1_meta_reddit.csv\n",
      "results/subnetworks\\Network_1_meta_reddit.csv  is finished\n",
      "Opening results/subnetworks\\Network_1_politics_news.csv\n",
      "results/subnetworks\\Network_1_politics_news.csv  is finished\n",
      "Opening results/subnetworks\\Network_1_right.csv\n",
      "results/subnetworks\\Network_1_right.csv  is finished\n",
      "Opening results/subnetworks\\Network_1_science_pseudoscience.csv\n",
      "results/subnetworks\\Network_1_science_pseudoscience.csv  is finished\n",
      "Opening results/subnetworks\\politics_news.csv\n",
      "results/subnetworks\\politics_news.csv  is finished\n",
      "Opening results/subnetworks\\right.csv\n",
      "received 403 HTTP response\n",
      "5l8040s comments was not accessible\n",
      "received 403 HTTP response\n",
      "55s74ns comments was not accessible\n",
      "received 403 HTTP response\n",
      "5ga3xhs comments was not accessible\n",
      "received 403 HTTP response\n",
      "59uh3ks comments was not accessible\n",
      "received 403 HTTP response\n",
      "59d2yvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6y0nr0s comments was not accessible\n",
      "received 403 HTTP response\n",
      "pwbdggs comments was not accessible\n",
      "Invalid URL: https://ww\n",
      "/r/DescentIntoTyranny/comments/5u8ygv/the_stingray_is_exactly_why_the_4th_amendment_was/ is not accessible\n",
      "received 403 HTTP response\n",
      "nhbxq7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "hv4k75s comments was not accessible\n",
      "received 403 HTTP response\n",
      "klrsr4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "s4oplrs comments was not accessible\n",
      "received 403 HTTP response\n",
      "i1k04zs comments was not accessible\n",
      "received 403 HTTP response\n",
      "ko45v8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "jbeky0s comments was not accessible\n",
      "received 403 HTTP response\n",
      "l5ew7os comments was not accessible\n",
      "received 403 HTTP response\n",
      "ihyb7hs comments was not accessible\n",
      "received 403 HTTP response\n",
      "lzrjj9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "iqgwbds comments was not accessible\n",
      "received 403 HTTP response\n",
      "qa6wyqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "k2nants comments was not accessible\n",
      "received 403 HTTP response\n",
      "hcx8sqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "iv0dh2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "s12cdzs comments was not accessible\n",
      "received 403 HTTP response\n",
      "pku22ds comments was not accessible\n",
      "received 403 HTTP response\n",
      "qbksxws comments was not accessible\n",
      "received 403 HTTP response\n",
      "iv4c9vs comments was not accessible\n",
      "received 403 HTTP response\n",
      "i1k04zs comments was not accessible\n",
      "received 403 HTTP response\n",
      "klqcses comments was not accessible\n",
      "received 403 HTTP response\n",
      "dfwbe6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "efar5ps comments was not accessible\n",
      "received 403 HTTP response\n",
      "e0s531s comments was not accessible\n",
      "received 403 HTTP response\n",
      "73638vs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6fcmsjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "71755bs comments was not accessible\n",
      "received 403 HTTP response\n",
      "5xcdmss comments was not accessible\n",
      "received 403 HTTP response\n",
      "5y6t6rs comments was not accessible\n",
      "received 403 HTTP response\n",
      "88mhnvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "5fjfvvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "amzz75s comments was not accessible\n",
      "received 403 HTTP response\n",
      "aij1mqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "aij1mqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6pa9rus comments was not accessible\n",
      "received 403 HTTP response\n",
      "6ok3czs comments was not accessible\n",
      "received 403 HTTP response\n",
      "7ahqrqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "7adg89s comments was not accessible\n",
      "received 403 HTTP response\n",
      "6nj8rps comments was not accessible\n",
      "received 403 HTTP response\n",
      "6n0deks comments was not accessible\n",
      "received 403 HTTP response\n",
      "6mk4ihs comments was not accessible\n",
      "received 403 HTTP response\n",
      "7dllnfs comments was not accessible\n",
      "received 403 HTTP response\n",
      "7adep4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "7a6qiys comments was not accessible\n",
      "received 403 HTTP response\n",
      "79yiths comments was not accessible\n",
      "received 403 HTTP response\n",
      "79lrpxs comments was not accessible\n",
      "received 403 HTTP response\n",
      "797d8ps comments was not accessible\n",
      "received 403 HTTP response\n",
      "78j1e7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "750863s comments was not accessible\n",
      "received 403 HTTP response\n",
      "728jots comments was not accessible\n",
      "received 403 HTTP response\n",
      "6zl44xs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6zl50es comments was not accessible\n",
      "received 403 HTTP response\n",
      "6xv8ges comments was not accessible\n",
      "received 403 HTTP response\n",
      "6wxqshs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6whrkgs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6vv6r7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "6jqsp1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "7elclxs comments was not accessible\n",
      "received 403 HTTP response\n",
      "5vozxws comments was not accessible\n",
      "received 403 HTTP response\n",
      "7dov9bs comments was not accessible\n",
      "received 403 HTTP response\n",
      "5rjidos comments was not accessible\n",
      "received 403 HTTP response\n",
      "6f01dms comments was not accessible\n",
      "received 403 HTTP response\n",
      "6whrkgs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6n0deks comments was not accessible\n",
      "received 403 HTTP response\n",
      "7ef3azs comments was not accessible\n",
      "received 403 HTTP response\n",
      "797d8ps comments was not accessible\n",
      "received 403 HTTP response\n",
      "6pkzuxs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6pa9rus comments was not accessible\n",
      "received 403 HTTP response\n",
      "5u3s7ks comments was not accessible\n",
      "received 403 HTTP response\n",
      "6o39zvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6kd3bts comments was not accessible\n",
      "received 403 HTTP response\n",
      "6fhbfbs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6euyzgs comments was not accessible\n",
      "received 403 HTTP response\n",
      "5wx1p9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "5wp7xss comments was not accessible\n",
      "received 403 HTTP response\n",
      "5w7jc0s comments was not accessible\n",
      "received 403 HTTP response\n",
      "5vwdkks comments was not accessible\n",
      "received 403 HTTP response\n",
      "5vw6cvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "5vfbrrs comments was not accessible\n",
      "received 403 HTTP response\n",
      "5v3595s comments was not accessible\n",
      "received 403 HTTP response\n",
      "6jqsp1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "6xv8ges comments was not accessible\n",
      "received 403 HTTP response\n",
      "632btjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "5rzpyts comments was not accessible\n",
      "received 403 HTTP response\n",
      "5rjidos comments was not accessible\n",
      "received 403 HTTP response\n",
      "c156kps comments was not accessible\n",
      "received 403 HTTP response\n",
      "9lut84s comments was not accessible\n",
      "received 403 HTTP response\n",
      "akpft1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "b8e67xs comments was not accessible\n",
      "received 403 HTTP response\n",
      "arp2o0s comments was not accessible\n",
      "received 403 HTTP response\n",
      "a6gjjms comments was not accessible\n",
      "received 403 HTTP response\n",
      "8ihf4as comments was not accessible\n",
      "received 403 HTTP response\n",
      "br6w4ls comments was not accessible\n",
      "received 403 HTTP response\n",
      "a46hlxs comments was not accessible\n",
      "received 403 HTTP response\n",
      "abahues comments was not accessible\n",
      "results/subnetworks\\right.csv  is finished\n",
      "Opening results/subnetworks\\science_pseudoscience.csv\n",
      "received 403 HTTP response\n",
      "52lks7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "529lkqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "51lyqcs comments was not accessible\n",
      "received 403 HTTP response\n",
      "51el5ws comments was not accessible\n",
      "received 403 HTTP response\n",
      "528af8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "51x6c4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "519c7xs comments was not accessible\n",
      "received 403 HTTP response\n",
      "52qeuos comments was not accessible\n",
      "received 403 HTTP response\n",
      "52bsq4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "51v13zs comments was not accessible\n",
      "received 403 HTTP response\n",
      "51l6g2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "518y2rs comments was not accessible\n",
      "received 403 HTTP response\n",
      "51x6cfs comments was not accessible\n",
      "received 403 HTTP response\n",
      "527a9qs comments was not accessible\n",
      "received 403 HTTP response\n",
      "50xraxs comments was not accessible\n",
      "received 403 HTTP response\n",
      "51ams3s comments was not accessible\n",
      "received 403 HTTP response\n",
      "512rins comments was not accessible\n",
      "received 403 HTTP response\n",
      "50wlf0s comments was not accessible\n",
      "received 403 HTTP response\n",
      "52n2fzs comments was not accessible\n",
      "received 403 HTTP response\n",
      "52jyebs comments was not accessible\n",
      "received 403 HTTP response\n",
      "52ept9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "5182khs comments was not accessible\n",
      "received 403 HTTP response\n",
      "530f4ls comments was not accessible\n",
      "received 403 HTTP response\n",
      "52ye9fs comments was not accessible\n",
      "received 403 HTTP response\n",
      "51yv29s comments was not accessible\n",
      "received 403 HTTP response\n",
      "513akys comments was not accessible\n",
      "received 403 HTTP response\n",
      "5195l4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "52w2jas comments was not accessible\n",
      "received 403 HTTP response\n",
      "51x9bws comments was not accessible\n",
      "received 403 HTTP response\n",
      "51c9rbs comments was not accessible\n",
      "received 403 HTTP response\n",
      "514175s comments was not accessible\n",
      "received 403 HTTP response\n",
      "50z9xzs comments was not accessible\n",
      "received 403 HTTP response\n",
      "50z5g8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "52nscfs comments was not accessible\n",
      "received 403 HTTP response\n",
      "527z66s comments was not accessible\n",
      "received 403 HTTP response\n",
      "51r830s comments was not accessible\n",
      "received 403 HTTP response\n",
      "5317gss comments was not accessible\n",
      "received 403 HTTP response\n",
      "522rems comments was not accessible\n",
      "received 403 HTTP response\n",
      "52l9qgs comments was not accessible\n",
      "received 403 HTTP response\n",
      "528dxos comments was not accessible\n",
      "received 403 HTTP response\n",
      "51l8nms comments was not accessible\n",
      "received 403 HTTP response\n",
      "5132x1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "52pnlgs comments was not accessible\n",
      "received 403 HTTP response\n",
      "52pzbjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "52n2ghs comments was not accessible\n",
      "received 403 HTTP response\n",
      "51qndgs comments was not accessible\n",
      "received 403 HTTP response\n",
      "51rfxds comments was not accessible\n",
      "received 403 HTTP response\n",
      "51bvaqs comments was not accessible\n",
      "results/subnetworks\\science_pseudoscience.csv  is finished\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "to_scan= [file for file in get_all_in_dir(\"results/subnetworks\")]\n",
    "for subreddit in to_scan:\n",
    "        comment_dict = dict()\n",
    "        print('Opening', subreddit)\n",
    "        try: \n",
    "\n",
    "                df = pd.read_csv(subreddit, encoding='utf8', on_bad_lines='skip')\n",
    "                \n",
    "                if os.path.isfile('comments_'+subreddit.split('\\\\')[1]):\n",
    "                        start = (pd.read_csv('comments_'+subreddit.split('\\\\')[1])['Source'].iloc[-1], pd.read_csv('comments_'+subreddit.split('\\\\')[1])['Target'].iloc[-1])\n",
    "                        start= int(df.loc[(df['Source'] == start[0]) & (df['Target'] == start[1])].index[0])\n",
    "                        if start >= len(df):\n",
    "                                continue        \n",
    "                        df = df[start+1:].dropna()\n",
    "        except Exception as e:\n",
    "               print('error',e,subreddit)\n",
    "               continue\n",
    "                \n",
    "        for row in df.to_dict(orient='records'):\n",
    "                if isinstance(row['Source'], float) or isinstance(row['Target'],float):\n",
    "                        continue\n",
    "                if 'https' in row['Source'] or 'https' in row['Target']:\n",
    "                        continue\n",
    "                try:\n",
    "                        list_post= row['Posts'][1:-1].split(',')\n",
    "                except Exception as e:\n",
    "                        continue\n",
    "                result = ''\n",
    "                num = 0\n",
    "                for post_url in list_post:\n",
    "                        if post_url == 0:\n",
    "                                continue\n",
    "                        post_url = post_url.replace(' ','')\n",
    "                        #tmp =  re.sub(\"\\[\\]\\'\", \"\", post_url) NON FUNZIONA E NON CAPISCO PERCHé\n",
    "                        if post_url[1:-1] in comment_dict.keys():\n",
    "                                result += comment_dict[post_url[1:-1]][0]\n",
    "                                num += comment_dict[post_url[1:-1]][1]\n",
    "                                continue\n",
    "                        try:\n",
    "                                post = reddit.submission(url=post_url[1:-1])\n",
    "                                try:\n",
    "                                        #This allows up to 1 reply to each post.\n",
    "                                        post.comments.replace_more(limit=1)\n",
    "                                except Exception as e:\n",
    "                                        print(e)\n",
    "                                        print(f'{post}s comments was not accessible')\n",
    "                                        continue\n",
    "                                num += len(post.comments)\n",
    "                                this_comment = ''\n",
    "                                if num > 500:\n",
    "                                        for comment in iterSample(post.comments, 500):\n",
    "                                                try:\n",
    "                                                        this_comment += comment.body\n",
    "                                                        for reply in comment.replies:\n",
    "                                                                this_comment+= reply.body\n",
    "                                                        this_comment=this_comment.replace(',','')\n",
    "                                                        this_comment=this_comment.replace('\\n','\\s')\n",
    "                                                except Exception as e:\n",
    "                                                        print(e)\n",
    "                                                        continue\n",
    "                                else:\n",
    "                                        for comment in post.comments:\n",
    "                                                try:\n",
    "                                                        this_comment += comment.body\n",
    "                                                        for reply in comment.replies:\n",
    "                                                                this_comment+= reply.body\n",
    "                                                        this_comment=this_comment.replace(',','')\n",
    "                                                        this_comment=this_comment.replace('\\n','\\s')\n",
    "\n",
    "                                                except Exception as e:\n",
    "                                                        print(e)\n",
    "                                                        continue\n",
    "                                comment_dict[post_url[1:-1]] = (this_comment,num)\n",
    "                                result += this_comment\n",
    "                        except Exception as e:\n",
    "                                print(e)\n",
    "                                print(post.permalink + ' is not accessible')\n",
    "                                continue                                         \n",
    "                res=pd.DataFrame.from_dict([{'Source':row['Source'], 'Target':row['Target'],'NumberComments': num,'Comments':result}])\n",
    "                if os.path.isfile('comments_'+subreddit.split('\\\\')[1]):\n",
    "                        res.to_csv('comments_'+subreddit.split('\\\\')[1],  encoding='utf8', mode='a', header=False)\n",
    "                else:\n",
    "                        res.to_csv('comments_'+subreddit.split('\\\\')[1],  encoding='utf8')\n",
    "                        \n",
    "        print(subreddit, ' is finished')\n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform the topic modeling: first we need to define the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print a csv for each crossposting between subreddits with the topic modeling of the common posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scan = [file for file in get_all_in_dir('comments')] #inserire iterazione su tutti i csv\n",
    "to_scan.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comments\\\\comments_conspiracies.csv', 'comments\\\\comments_left.csv', 'comments\\\\comments_left_right_bernie.csv', 'comments\\\\comments_meta_reddit.csv', 'comments\\\\comments_politics_news.csv', 'comments\\\\comments_right.csv', 'comments\\\\comments_science_pseudoscience.csv']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 62\n",
      "19 19\n",
      "13 13\n",
      "16 16\n",
      "38 38\n",
      "10 10\n",
      "11 11\n",
      "6 6\n",
      "10 10\n",
      "17 17\n",
      "16 16\n",
      "39 39\n",
      "10 10\n",
      "25 25\n",
      "75 75\n",
      "36 36\n",
      "54 54\n",
      "7 7\n",
      "3 3\n",
      "115 115\n",
      "25 25\n",
      "42 42\n",
      "3 3\n",
      "46 46\n",
      "51 51\n",
      "21 21\n",
      "194 194\n",
      "83 83\n",
      "115 115\n",
      "12 12\n",
      "24 24\n",
      "19 19\n",
      "8 8\n",
      "12 12\n",
      "72 72\n",
      "30 30\n",
      "13 13\n",
      "28 28\n",
      "31 31\n",
      "6 6\n",
      "21 21\n",
      "49 49\n",
      "4 4\n",
      "15 15\n",
      "12 12\n",
      "4 4\n",
      "42 42\n",
      "6 6\n",
      "25 25\n",
      "41 41\n",
      "13 13\n",
      "33 33\n",
      "45 45\n",
      "50 50\n",
      "31 31\n",
      "30 30\n",
      "6 6\n",
      "41 41\n",
      "21 21\n",
      "6 6\n",
      "5 5\n",
      "34 34\n",
      "23 23\n",
      "24 24\n",
      "38 38\n",
      "26 26\n",
      "29 29\n",
      "24 24\n",
      "29 29\n",
      "26 26\n",
      "39 39\n",
      "18 18\n",
      "9 9\n",
      "15 15\n",
      "7 7\n",
      "29 29\n",
      "18 18\n",
      "22 22\n",
      "22 22\n",
      "24 24\n",
      "32 32\n",
      "29 29\n",
      "11 11\n",
      "27 27\n",
      "13 13\n",
      "16 16\n",
      "27 27\n",
      "38 38\n",
      "29 29\n",
      "19 19\n",
      "14 14\n",
      "41 41\n",
      "43 43\n",
      "42 42\n",
      "5 5\n",
      "19 19\n",
      "14 14\n",
      "26 26\n",
      "31 31\n",
      "21 21\n",
      "16 16\n",
      "40 40\n",
      "3 3\n",
      "24 24\n",
      "8 8\n",
      "5 5\n",
      "7 7\n",
      "9 9\n",
      "12 12\n",
      "24 24\n",
      "12 12\n",
      "31 31\n",
      "34 34\n",
      "20 20\n",
      "39 39\n",
      "9 9\n",
      "3 3\n",
      "44 44\n",
      "6 6\n",
      "8 8\n",
      "15 15\n",
      "34 34\n",
      "66 66\n",
      "57 57\n",
      "15 15\n",
      "47 47\n",
      "13 13\n",
      "55 55\n",
      "27 27\n",
      "61 61\n",
      "3 3\n",
      "3 3\n",
      "16 16\n",
      "43 43\n",
      "8 8\n",
      "4 4\n",
      "18 18\n",
      "4 4\n",
      "57 57\n",
      "4 4\n",
      "27 27\n",
      "18 18\n",
      "13 13\n",
      "8 8\n",
      "31 31\n",
      "16 16\n",
      "12 12\n",
      "5 5\n",
      "7 7\n",
      "14 14\n",
      "7 7\n",
      "5 5\n",
      "9 9\n",
      "94 94\n",
      "7 7\n",
      "28 28\n",
      "29 29\n",
      "16 16\n",
      "24 24\n",
      "4 4\n",
      "40 40\n",
      "22 22\n",
      "61 61\n",
      "5 5\n",
      "5 5\n",
      "84 84\n",
      "14 14\n",
      "26 26\n",
      "3 3\n",
      "21 21\n",
      "11 11\n",
      "14 14\n",
      "7 7\n",
      "8 8\n",
      "8 8\n",
      "4 4\n",
      "35 35\n",
      "4 4\n",
      "9 9\n",
      "22 22\n",
      "69 69\n",
      "5 5\n",
      "35 35\n",
      "23 23\n",
      "11 11\n",
      "14 14\n",
      "48 48\n",
      "33 33\n",
      "31 31\n",
      "59 59\n",
      "59 59\n",
      "57 57\n",
      "36 36\n",
      "55 55\n",
      "58 58\n",
      "61 61\n",
      "36 36\n",
      "4 4\n",
      "58 58\n",
      "17 17\n",
      "25 25\n",
      "88 88\n",
      "5 5\n",
      "5 5\n",
      "31 31\n",
      "65 65\n",
      "11 11\n",
      "30 30\n",
      "31 31\n",
      "24 24\n",
      "53 53\n",
      "53 53\n",
      "48 48\n",
      "5 5\n",
      "43 43\n",
      "28 28\n",
      "16 16\n",
      "62 62\n",
      "41 41\n",
      "19 19\n",
      "77 77\n",
      "51 51\n",
      "10 10\n",
      "34 34\n",
      "36 36\n",
      "52 52\n",
      "26 26\n",
      "80 80\n",
      "25 25\n",
      "38 38\n",
      "28 28\n",
      "58 58\n",
      "55 55\n",
      "29 29\n",
      "56 56\n",
      "45 45\n",
      "6 6\n",
      "71 71\n",
      "24 24\n",
      "3 3\n",
      "18 18\n",
      "40 40\n",
      "24 24\n",
      "72 72\n",
      "23 23\n",
      "35 35\n",
      "7 7\n",
      "45 45\n",
      "7 7\n",
      "19 19\n",
      "23 23\n",
      "9 9\n",
      "24 24\n",
      "69 69\n",
      "7 7\n",
      "46 46\n",
      "75 75\n",
      "54 54\n",
      "29 29\n",
      "29 29\n",
      "35 35\n",
      "21 21\n",
      "21 21\n",
      "55 55\n",
      "6 6\n",
      "43 43\n",
      "21 21\n",
      "86 86\n",
      "11 11\n",
      "21 21\n",
      "30 30\n",
      "12 12\n",
      "20 20\n",
      "23 23\n",
      "18 18\n",
      "32 32\n",
      "8 8\n",
      "20 20\n",
      "6 6\n",
      "27 27\n",
      "3 3\n",
      "20 20\n",
      "26 26\n",
      "25 25\n",
      "16 16\n",
      "20 20\n",
      "33 33\n",
      "5 5\n",
      "3 3\n",
      "23 23\n",
      "37 37\n",
      "13 13\n",
      "17 17\n",
      "35 35\n",
      "33 33\n",
      "24 24\n",
      "13 13\n",
      "19 19\n",
      "11 11\n",
      "26 26\n",
      "9 9\n",
      "7 7\n",
      "13 13\n",
      "14 14\n",
      "19 19\n",
      "32 32\n",
      "35 35\n",
      "36 36\n",
      "31 31\n",
      "20 20\n",
      "3 3\n",
      "29 29\n",
      "36 36\n",
      "18 18\n",
      "8 8\n",
      "11 11\n",
      "21 21\n",
      "34 34\n",
      "40 40\n",
      "23 23\n",
      "6 6\n",
      "41 41\n",
      "15 15\n",
      "26 26\n",
      "5 5\n",
      "23 23\n",
      "4 4\n",
      "10 10\n",
      "29 29\n",
      "17 17\n",
      "23 23\n",
      "10 10\n",
      "20 20\n",
      "12 12\n",
      "13 13\n",
      "79 79\n",
      "18 18\n",
      "17 17\n",
      "48 48\n",
      "17 17\n",
      "11 11\n",
      "11 11\n",
      "28 28\n",
      "8 8\n",
      "18 18\n",
      "30 30\n",
      "30 30\n",
      "13 13\n",
      "31 31\n",
      "16 16\n",
      "4 4\n",
      "19 19\n",
      "24 24\n",
      "34 34\n",
      "40 40\n",
      "20 20\n",
      "22 22\n",
      "17 17\n",
      "9 9\n",
      "46 46\n",
      "7 7\n",
      "5 5\n",
      "15 15\n",
      "11 11\n",
      "11 11\n",
      "28 28\n",
      "33 33\n",
      "7 7\n",
      "12 12\n",
      "29 29\n",
      "12 12\n",
      "30 30\n",
      "14 14\n",
      "4 4\n",
      "19 19\n",
      "20 20\n",
      "38 38\n",
      "34 34\n",
      "21 21\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "avoid = set()\n",
    "for file in to_scan:\n",
    "    if file in avoid:\n",
    "        continue\n",
    "    df = pd.read_csv(file, encoding='utf8').drop_duplicates()\n",
    "    if any(char.isdigit() for char in file):\n",
    "        filename = file.split('\\\\')[1]\n",
    "        filename = re.sub('\\d|\\.csv', '', filename)\n",
    "        for el in to_scan:\n",
    "            if filename in el:\n",
    "                avoid.add(el)\n",
    "                df.append(pd.read_csv(el, encoding='utf8').sort_values('Source'))\n",
    "    df.sort_values('Source',inplace=True)\n",
    "    length = len(df)\n",
    "    df.reset_index(inplace=True)\n",
    "    index = [df['Target']]\n",
    "    heat = pd.DataFrame(index = index)\n",
    "    targets = set(df['Source'])\n",
    "    for column in targets:\n",
    "        if 'https' in column or '/' in column:\n",
    "            continue\n",
    "        to_add = []\n",
    "        with_target=df[df['Source']==column]\n",
    "        start = df[df['Source']==column].first_valid_index()\n",
    "        end = df[df['Source']==column].last_valid_index()\n",
    "        if start > 0:\n",
    "            to_add=['NaN' for i in range(start)]\n",
    "            for row in with_target.to_dict(orient='records'):\n",
    "                to_add.append(row['Comments'])\n",
    "        else:\n",
    "            for row in with_target.to_dict(orient='records'):\n",
    "                to_add.append(row['Comments'])\n",
    "        if end < length-1:\n",
    "            to_add.extend('NaN' for i in range(end+1,length))\n",
    "        heat[column] = to_add\n",
    "    heat.to_csv('heat/heat_'+file.split('\\\\')[1], encoding='utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor for\n",
    "for file in to_scan:\n",
    "    df = pd.read_csv(file, encoding='utf8')\n",
    "    origin, id = key.split('@ ')\n",
    "    if '1st_level_2' in origin:\n",
    "        origin = origin.split('1st_level_2')[1]\n",
    "        origin = origin.replace('\\\\','')\n",
    "        origin = origin.replace('.csv','')\n",
    "    if '1st_level' in origin:\n",
    "        origin = origin.split('1st_level')[1]\n",
    "        origin = origin.replace('\\\\','')\n",
    "        origin = origin.replace('.csv','')\n",
    "    if '2nd_level' in origin:\n",
    "        origin = origin.split('2nd_level')[1]\n",
    "        origin = origin.replace('\\\\','')\n",
    "        origin = origin.replace('.csv','')\n",
    "\n",
    "    id=id[id.index('/r/')+3:]\n",
    "    if 'x0' in key:\n",
    "        key = key.replace('x0','')\n",
    "\n",
    "    # Convert to list\n",
    "    data = []\n",
    "    mt = top_comments[key]\n",
    "    if len(mt) > 4:\n",
    "        for i in mt:\n",
    "            data.append(reddit.comment(i))\n",
    "        # Remove tags from comments\n",
    "        try:\n",
    "            data = [re.sub('\\S*@\\S*\\s?', '', sent.body) for sent in data]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        # Remove new line characters\n",
    "        data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "        # Remove distracting single quotes\n",
    "        data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "        # Tokenizing the text\n",
    "        data_words = list(sent_to_words(data))\n",
    "\n",
    "        # Build the bigram and trigram models\n",
    "        bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "        trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "        # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "        bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "        trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "        # Remove Stop Words\n",
    "        data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "        # Form Bigrams\n",
    "        data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "        # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "        # python3 -m spacy download en\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "        # Do lemmatization keeping only noun, adj, vb, adv\n",
    "        data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "        # Create Dictionary\n",
    "        id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "        # Create Corpus\n",
    "        texts = data_lemmatized\n",
    "\n",
    "        # Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        # Human readable format of corpus (term-frequency)\n",
    "        [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "        if corpus != [] and corpus != [[]]:\n",
    "            # Build LDA model\n",
    "            lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                    id2word=id2word,\n",
    "                                                    num_topics=10, \n",
    "                                                    random_state=100,\n",
    "                                                    update_every=1,\n",
    "                                                    chunksize=100,\n",
    "                                                    passes=10,\n",
    "                                                    alpha='auto',\n",
    "                                                    per_word_topics=True)\n",
    "\n",
    "            # Print the Keyword in the 10 topics\n",
    "            \n",
    "            index = []\n",
    "            topics = {}\n",
    "            list_topics=lda_model.print_topics()\n",
    "            for i in list_topics:\n",
    "                for value in range(len(list_topics)):\n",
    "                    to_add = []\n",
    "                    for el in list_topics[value][1].split('+'):\n",
    "                        start = el.index('\"')\n",
    "                        word = el[start+1:-2]             \n",
    "                        to_add.append(word)\n",
    "                    topics[value] = to_add\n",
    "\n",
    "            \n",
    "            df_topic = pd.DataFrame(topics)\n",
    "            done = pd.read_csv('done.csv', encoding='utf8')\n",
    "            done.insert(0,key,'NaN')\n",
    "            done.to_csv('done.csv' ,encoding='utf8', mode='w')\n",
    "\n",
    "            for row in df_topic.index:\n",
    "                index.append(origin)\n",
    "            df_topic['origin'] = index\n",
    "            if os.path.isfile(r\"results/topic_models/\"+ id + '.csv'):\n",
    "                df_topic.set_index('origin', inplace=True)\n",
    "                df_topic.to_csv(r\"results/topic_models/\"+ id + '.csv', mode='a', encoding = 'utf8', header=False)\n",
    "            else:\n",
    "                df_topic.set_index('origin', inplace=True)\n",
    "                df_topic.to_csv(r\"results/topic_models/\"+ id + '.csv', encoding = 'utf8')\n",
    "            #pprint(lda_model.print_topics())\n",
    "        else: \n",
    "            print (key + \" is done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting csv file will contain the topics of crossposts.\n",
    "### Sentiment Analysis\n",
    "Sentiment analysis is the computational study of people's emotions expressed in text. In our case we used the popular VADER sentiment analyser, an analyser especially created for social networks (in particular, it was based off Twitter). The result of each comment's analysis will be a number between -1 and 1, depending on whether the comment is perceived as negative or positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sent = [file for file in get_all_in_dir('heat')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sent = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in to_sent:\n",
    "    df = pd.read_csv(file, encoding = 'utf8').dropna(how='all')\n",
    "    out = pd.DataFrame(index = [el for el in df['Target'].to_list() if not 'https' in el and not '/' in el])\n",
    "    for column in df.columns[1:]:\n",
    "        for el in df[column]:\n",
    "            if isinstance(el, float):\n",
    "                out[column] = 0\n",
    "                continue\n",
    "            out[column] = sent.polarity_scores(el)['compound']\n",
    "    out.to_csv('map'+file.split('\\\\')[1], encoding='utf8')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per plottare con pandas etc: inverti colonna, rendi int, droppa colonne inutili, and go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[vedi qui](https://stackabuse.com/rotate-axis-labels-in-matplotlib/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consp = pd.read_csv('results/1st_level/conspiracy.csv', encoding = 'utf8')\n",
    "consp = consp.drop('Unnamed: 0', axis=1)\n",
    "consp = consp.drop('Unnamed: 0.1', axis=1)\n",
    "consp = consp.drop('Unnamed: 0.0.1', axis=1)\n",
    "\n",
    "consp = consp.transpose()\n",
    "consp= consp.drop(0, axis=1)\n",
    "consp = consp.rename({1:'crossposts'}, axis=1)\n",
    "consp = consp.astype(int)\n",
    "consp = consp.sort_values('crossposts', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y=consp.crossposts,x=consp.index)\n",
    "plt.xticks(rotation = 90) # Rotates X-Axis Ticks by 45-degrees\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d65f8af9b1347936d5c0a715a1a101b7602968bee42a1bc2161adfc924f1cbb0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
