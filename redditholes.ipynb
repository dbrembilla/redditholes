{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RedditHoles - A study in  internet Rabbit Holes\n",
    "\n",
    "## An introduction to Reddit\n",
    "\n",
    "Reddit is a social media platform organised in communities, rather than individual connections. This makes the experience on the platform quite different from other Social Networks and closer, in a way, to the one of forums.  \n",
    "\n",
    "### u/User\n",
    "A user (also called redditor, usually referred as “u/” followed by the username) can make a post (also called a submission); posts (also called submissions) can be links, videos, pictures, polls or text. The OP (Original Poster) as well as other users can comment and vote (either upvote or downvote) the post if they find it interesting (so it’s not exactly like like/dislike features of other Social Networks). Finally, users can give posts awards by paying for reddit coins; these recognise the contributions of a post or comment. There are hundreds of these, from a generic gold or silver award to some following internet lingo, such as the “F” award (used ironically to ‘pay respect’). \n",
    "\n",
    "Users can mark their posts as spoilers and multiple types of flairs and markings such as OC (Original Content), Spoilers, +18 and so on. Some subreddits might have rules for posting. \n",
    "\n",
    "Users can be humans or bots. Bots have multiple uses, from auto moderation, to answering with quotes from movies and books, waving flags to other utilities and fun uses. \n",
    "#### The average redditor\n",
    "\n",
    "Reddit users, according to [data from the site itself](https://www.redditinc.com/advertising/audience), are more than 50 million with more than 100 thousand communities. They are mostly male (56%) and have between 18 and 34 year old (58%). \n",
    "\n",
    "### r/Subreddits\n",
    "\n",
    "Reddit is structured in subreddits, communities that group in various ways the interested users. Subreddits can vary from cute pets, to political parties, to recipe advice. Subreddits are usually referred to as “r/” followed by the name of the subreddits (in our case, we will study “r/conspiracy” and related subreddits). \n",
    "\n",
    "Subreddits can vary significantly. They can have rules for posting, different levels of moderation and bot acceptance, all depending from the nature of the subreddit. Rules of subreddits can have different nature; some subreddits may require the content of a post to be marked, some require sources to be linked, some can have no rules at all. This ambiguity makes Reddit a much more decentralised and open Social Network in which some of the cut down on fake news and trolls which happened on sites such as Facebook or Twitter has not yet happened at the same level. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Reddit Data\n",
    "In order to access data from the Reddit API, we used the [`praw` python library](https://github.com/praw-dev/praw). \n",
    "In order to use that, we need:\n",
    "- a Reddit account.\n",
    "- a Reddit app ([here](https://www.reddit.com/prefs/apps) you can create one). Here you'll find the client id under the app name once you create it, as well as the client secret and the user agent, which is your app name.\n",
    "\n",
    "Once we have those, we can access reddit from the python terminal by creating a reddit instance in praw (it is also possible to access as a user by adding username and account if you want to use it in write mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install praw\n",
    "#!pip install pandas\n",
    "import praw\n",
    "reddit = praw.Reddit(client_id = \"CMO4Nf8Dpd3YE_lftmqnHg\", client_secret= \"79YxaeUzQYqtVpvev6SbWbCMvF70-g\", user_agent= \"little_digging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can access information about subreddits, posts and users. As an example, we can access the top 5 posts of all time on Reddit, with the information about the author and the upvote ratio and datat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reddit.subreddit('all').top(limit=5): #this will print the top posts all time\n",
    "    print(i.title + ' Author: ' + i.author.name +  ' Link: https://www.reddit.com' + i.permalink + ' Subreddit: r/' + i.subreddit.display_name + ' Upvote Ratio: ' + str(i.upvote_ratio) + ' Date (UTC Format): '+str(i.created_utc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Reddit, starting from r/conspiracy\n",
    "\n",
    "Our objective is to watch how a piece of news or a post is shared between different subreddits. While most social network would measure shares of a post, Reddit is built in a way that if a link is shared in the platform, it is possible to retrieve how much the original link is shared through subreddits. As an example, if an image from imgur is shared, we can use its url to search for the same object on Reddit.\n",
    "This happens because, for the most part, Reddit is used to comment news and multimedia in communities, which often present a common worldview (e.g. subbreddits made by people with same political views).\n",
    "We started with the [r/conspiracy](https://www.reddit.com/r/conspiracy/)'s posts, in particular the top 5000 posts all time. We opted for the top posts all time because the other types of ranking are time-bound and we wanted to watch the overall transmission of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "post_list=list()\n",
    "subreddit_list = list()\n",
    "conspiracy_dict=dict()\n",
    "\n",
    "for i in reddit.subreddit(\"conspiracy\").top(limit=5000): \n",
    "    post_list.append(i.url) # This may seem counterintuitive, but in praw's terms this is the original link of the resource inside the post.\n",
    "\n",
    "\n",
    "for post in post_list:\n",
    "    for repost in reddit.subreddit('all').search('url:'+post): # This function searches for the original post's element\n",
    "        subreddit_url = str(repost.subreddit)\n",
    "        subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "        if subreddit_url in conspiracy_dict.keys():\n",
    "            conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "            conspiracy_dict[subreddit_url][1][0] +=1\n",
    "        else:\n",
    "            conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "            conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "            \n",
    "                \n",
    "\n",
    "\n",
    "df = pd.DataFrame(conspiracy_dict)\n",
    "df.to_csv(r'results/conspiracy_data/conspiracy.csv',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realised after the fact that this method also got the reposts inside the same subreddit. After manually cleaning the csv in this instance, we proceeded to remove this problem in the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a network of subreddits\n",
    "\n",
    "After scraping r/conspiracy, we moved to the neighbouring subreddits. What this will do is creating a network of shared posts between subreddits.\n",
    "\n",
    "First we need to look at all the files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_in_dir(dir):\n",
    "    for filename in os.listdir(dir):\n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f) and f[-4:] == \".csv\":\n",
    "            yield f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can scrape the other subreddits that had more than 5 posts in common with r/conspiracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/conspiracy_data/conspiracy.csv')\n",
    "id_to_analyse = []\n",
    "for column in df1.columns:\n",
    "    value = df1[column][1][1:-1]\n",
    "    if int(value) >= 5:\n",
    "        ind = column.index('/r/')\n",
    "        id = column[ind+3:]\n",
    "        id_to_analyse.append(id)\n",
    "        \n",
    "\n",
    "for subreddit in id_to_analyse:\n",
    "    post_list=list()\n",
    "    subreddit_list = list()\n",
    "    conspiracy_dict=dict() \n",
    "    for i in reddit.subreddit(subreddit).top(limit=5000):\n",
    "        post_list.append((i.title, i.score, i.url))\n",
    "\n",
    "\n",
    "\n",
    "    for post in post_list:\n",
    "        for repost in reddit.subreddit('all').search('url:'+post[2]):\n",
    "            if repost.subreddit_id != \"t5_\"+reddit.subreddit(subreddit).id: #cosa facciamo?\n",
    "                subreddit_url = str(repost.subreddit)\n",
    "                subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "                if subreddit_url in conspiracy_dict.keys():\n",
    "                    conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                    conspiracy_dict[subreddit_url][1][0] +=1\n",
    "                else:\n",
    "                    conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "                    conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "    df = pd.DataFrame(conspiracy_dict)\n",
    "    df.to_csv(r'results/1st_level/'+subreddit+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the cycle one more time, but this time with just the top 500 posts, in order to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now opening results/1st_level_2\\Kossacks_for_Sanders.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20264/523188709.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m                         \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/r/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                         \u001b[0mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "top_comments_list = dict()\n",
    "to_scan = [file for file in get_all_in_dir(\"results/1st_level_2\")]\n",
    "to_avoid= [file.split('\\\\')[1] for file in get_all_in_dir(\"results/1st_level\")]\n",
    "to_avoid.extend(file.split('\\\\')[1] for file in get_all_in_dir(\"results/1st_level_2\"))\n",
    "to_avoid.extend(file.split('\\\\')[1] for file in get_all_in_dir(\"results/2nd_level\"))\n",
    "\n",
    "for subr in to_scan:\n",
    "        id_to_analyse = []\n",
    "        print(f'now opening {subr}')\n",
    "        try:\n",
    "                df1 = pd.read_csv(subr)\n",
    "        except:\n",
    "                pass\n",
    "        try:\n",
    "                df1 = pd.read_csv(subr, encoding='utf8')\n",
    "        except:\n",
    "                print(f'unable to open {subr}')\n",
    "                continue\n",
    "        for column in df1.columns:\n",
    "                try: \n",
    "                        value = df1[column][1][1:-1]\n",
    "                except:\n",
    "                        continue\n",
    "                if int(value) >= 5:\n",
    "                        ind = column.index('/r/')\n",
    "                        id = column[ind+3:]\n",
    "                        if not id + '.csv' in to_avoid:\n",
    "                                try:\n",
    "                                        post_list=list()\n",
    "                                        subreddit_list = list()\n",
    "                                        conspiracy_dict=dict() \n",
    "                                        for i in reddit.subreddit(id).top(limit=500):\n",
    "                                                post_list.append((i.title, i.score, i.url))\n",
    "\n",
    "                                        for post in post_list:\n",
    "                                                for repost in reddit.subreddit('all').search('url:'+post[2]):\n",
    "                                                        if repost.subreddit_id != \"t5_\"+reddit.subreddit(id).id: #cosa facciamo?\n",
    "                                                                subreddit_url = str(repost.subreddit)\n",
    "                                                                subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "                                                        else:\n",
    "                                                                continue\n",
    "                                                        if subreddit_url in conspiracy_dict.keys():\n",
    "                                                                conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                                                                conspiracy_dict[subreddit_url][1][0] +=1\n",
    "                                                        else:\n",
    "                                                                conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "                                                                conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                                        \n",
    "                                        df = pd.DataFrame(conspiracy_dict)\n",
    "                                        df.to_csv(r'results/2nd_level/'+id+'.csv')\n",
    "                                        with open(\"results/2nd_level/done_2.txt\",'a', encoding = \"utf-8\") as text_note:\n",
    "                                                text_note.write(id + \"\\n\")\n",
    "                                                text_note.close()\n",
    "                                        \n",
    "                                except Exception as E:\n",
    "                                        print(E)\n",
    "                                        with open(\"results/2nd_level/error_2.txt\",'a', encoding = \"utf-8\") as text_note:\n",
    "                                                text_note.write(id + \"\\n\")\n",
    "                                                text_note.close()\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "In order to perform more efficiently the operations of data representation we decided two perform two operations:\n",
    "1. We turned the number of crossposts into integers.\n",
    "2. We removed subreddits with less than 5 posts in common.\n",
    "These was in order to have cleaner and more usable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing parentheses and connections with less than 5 reposts\n",
    "import pandas as pd\n",
    "datasets= [file for file in get_all_in_dir(\"results/1st_level\")]\n",
    "datasets.extend(file for file in get_all_in_dir(\"results/1st_level_2\"))\n",
    "datasets.extend(file for file in get_all_in_dir(\"results/2nd_level\"))\n",
    "datasets.extend(\"results/conspiracy_data/conspiracy_top_url.csv\")\n",
    "\n",
    "for file in datasets:\n",
    "    try:\n",
    "        df = pd.read_csv(file, sep=',', on_bad_lines='skip', encoding='utf8')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        df = pd.read_csv(file, on_bad_lines='skip', encoding='latin')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(file, 'has problems in formatting')\n",
    "        continue\n",
    "    for col in df.columns:\n",
    "        if \"u/\" in col or 'u_' in col: #Sometimes users end up in the columns. Remove them. \n",
    "            df.drop(col, inplace=True, axis=1)\n",
    "        else:\n",
    "            try:\n",
    "                \n",
    "               if isinstance(df[col][1], str): \n",
    "                    try:\n",
    "                        df[col][1]=int(df[col][1][1:-1])\n",
    "                    except:\n",
    "                        continue\n",
    "                    if df[col][1]<5:\n",
    "                        df.drop(col, inplace=True, axis=1)\n",
    "            except:\n",
    "                df.drop(col, inplace=True, axis=1)\n",
    "    df.to_csv(file, encoding=\"utf8\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data from Reddit, we can build a network of shared posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Network of crossposts\n",
    "By using Gephi, we plotted the network of crosspost. By doing this, we found some communities of subreddits that present a significant number of crossposts.\n",
    "1. <b>Crypto, Tech and other news</b>, with a part dedicated to tech (BitcoinAll, Mistifront, Libertarian, Technews and brprogramming) and another about politics and conspiracies (conspiracy, politics...).\n",
    "2. <b>MetaReddit</b>, subreddits that share a meta-reddit approach (e.g. bots collecting stories from everywhere or comments about reddit), that are also very interlinked with generalist subreddits (such as r/pics). Among these are mistyfront, SubredditNN, Blackout2015. This is very intelinked with tech and general politics.  \n",
    "3. <b>Politics/News</b> with various news and political subreddits, from either party or with no party affiliation, but also with a significant conspiracy news subgroups. Among these are anythingGoesNews, ConspiracyII, Coronavirus, infrasociology\n",
    "4. <b>Leftist Politics</b> centred around WayOfTheBern, LateStageCapitalism, SocialismAndVeganism, but also TrueReddit.\n",
    "5. <b>Bernie Sanders</b> - Started from StillSandersForPresident, WayOfTheBern, RealBlueMidterm (central to other minor subreddits) A significant group of subreddits all sharing the topic of popularising the figure of Bernie Sanders. The main gateways seem to be NEwYorkSanders,FLoridaforSanders ad CaliforniaFOrSanders.\n",
    "6. <b>Conservative/Libertarian</b> subreddits from the right and alt-right, with Conservative, Libertarian, Descent into tyranny but also  news sources such as Worldpolitics and alternative news subreddits such as Censorship \n",
    "7. <b>Science and Pseudoscience</b>, subreddits discussing both scientific news(environment, AutoNewspaper), views of the future(futurology) but also pseudoscience (ScienceUncensored, DebateVaccines)\n",
    "8. <b>Information</b> a subnetwork connected both to general information subreddits (todayilearned) and classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the network\n",
    "datasets= [file for file in get_all_in_dir(\"results/1st_level\")]\n",
    "datasets.extend(file for file in get_all_in_dir(\"results/1st_level_2\"))\n",
    "datasets.extend(file for file in get_all_in_dir(\"results/2nd_level\"))\n",
    "\n",
    "network_list = list()\n",
    "\n",
    "for subr in datasets:\n",
    "        #print(f'now opening {subr}')\n",
    "        try:\n",
    "                df1 = pd.read_csv(subr)\n",
    "        except:\n",
    "                pass\n",
    "        try:\n",
    "                df1 = pd.read_csv(subr, encoding='utf8')\n",
    "        except:\n",
    "                print(f'unable to open {subr}')\n",
    "                continue\n",
    "        sub_name=subr[subr.find(\"\\\\\")+1:-4]\n",
    "        for column in df1.columns:\n",
    "                try: \n",
    "                        value = column\n",
    "                except:\n",
    "                        continue\n",
    "                if column [:7] != \"Unnamed\":\n",
    "                        col_name = column[column.find(\"/r/\")+3:]\n",
    "                        try:\n",
    "                                network_list.append((sub_name, col_name, df1[column][1]))\n",
    "                        except Exception as E:\n",
    "                                print(E) \n",
    "                else: \n",
    "                        continue\n",
    "print(network_list)\n",
    "\n",
    "'''' with open('Network.csv', 'w') as f:\n",
    "      \n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "      \n",
    "    write.writerow([\"Source\", \"Target\", \"Weight\"])\n",
    "    write.writerows(network_list)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUI DOBBIAMO RIVEDERE L'ORGANIZZAZIONE; deve esserci un csv che contiene i post in comune tra csv1 e csv2, dicendo da dove arrivano. Questo ci permetterà di confrontare i risultanti.\n",
    "## Analysing the comments\n",
    "A part of our inquiry involves the kind of language that redditors use on the website in response to posts. In order to this, we employed two techniques that allow us to find out the nature of a text: Topic Modelling and Sentiment Analysis\n",
    "\n",
    "### Topic modelling\n",
    "Topic modelling is a machine learning technique that tries to predict the distribution of abstract topics in a text, and thus reveal the hidden semantic structures within it. In particular, we used Latent Dirichlet Allocation (LDA) method. We adapted the method used in [here](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#1introduction).\n",
    "The libraries used are <code>[nltk](https://www.nltk.org/), [gensim](https://github.com/RaRe-Technologies/gensim/), [spacy](https://spacy.io/), [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/readme.html)</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "#!pip install gensim\n",
    "#!pip install spacy\n",
    "#!pip install pyLDAvis\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['re', 'edit']) \n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as genmodels  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "#python -m spacy download en_core_web_sm\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of comments can be particularly big, we used this code snippet to get a random sample from bigger comment sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def iterSample(iterable, samplesize):\n",
    "    results = []\n",
    "    for i, v in enumerate(iterable):\n",
    "        r = random.randint(0, i)\n",
    "        if r < samplesize:\n",
    "            if i < samplesize:\n",
    "                results.insert(r, v) # add first samplesize items in random order\n",
    "            else:\n",
    "                results[r] = v # at a decreasing rate, replace random items\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the dataset and the comments from the posts in common between subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening results/subnetworks\\bernie.csv\n",
      "results/subnetworks\\bernie.csv  is finished\n",
      "Opening results/subnetworks\\bitcoins.csv\n",
      "results/subnetworks\\bitcoins.csv  is finished\n",
      "Opening results/subnetworks\\conspiracies.csv\n",
      "received 403 HTTP response\n",
      "git9wvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "itzolhs comments was not accessible\n",
      "received 403 HTTP response\n",
      "gaup9ks comments was not accessible\n",
      "received 403 HTTP response\n",
      "jjrofws comments was not accessible\n",
      "received 403 HTTP response\n",
      "fyk5h8s comments was not accessible\n",
      "results/subnetworks\\conspiracies.csv  is finished\n",
      "Opening results/subnetworks\\left.csv\n",
      "received 403 HTTP response\n",
      "a72ae4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "amrrt8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "bcd3urs comments was not accessible\n",
      "received 403 HTTP response\n",
      "af0zmhs comments was not accessible\n",
      "received 403 HTTP response\n",
      "a4wgq7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "cdjdpqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "azjrzus comments was not accessible\n",
      "received 403 HTTP response\n",
      "ac4x0ns comments was not accessible\n",
      "received 403 HTTP response\n",
      "2wyu08s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2emq4ys comments was not accessible\n",
      "received 403 HTTP response\n",
      "2wiq8es comments was not accessible\n",
      "received 403 HTTP response\n",
      "2cwp9xs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2sbb50s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2bq31ss comments was not accessible\n",
      "received 403 HTTP response\n",
      "2biu3es comments was not accessible\n",
      "received 403 HTTP response\n",
      "2az986s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2tq6c2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2sn7xcs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2rxugjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2obin0s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2x89yos comments was not accessible\n",
      "received 403 HTTP response\n",
      "2wescqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2k0rb9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2dcm4ds comments was not accessible\n",
      "received 403 HTTP response\n",
      "2a7lrcs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2vj4q1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2lk7ths comments was not accessible\n",
      "received 403 HTTP response\n",
      "2frbrks comments was not accessible\n",
      "received 403 HTTP response\n",
      "2c8qhps comments was not accessible\n",
      "received 403 HTTP response\n",
      "2ba0tes comments was not accessible\n",
      "received 403 HTTP response\n",
      "2ayspss comments was not accessible\n",
      "received 403 HTTP response\n",
      "2ada6is comments was not accessible\n",
      "received 403 HTTP response\n",
      "2jziccs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2wvxows comments was not accessible\n",
      "received 403 HTTP response\n",
      "2vrioos comments was not accessible\n",
      "received 403 HTTP response\n",
      "2ligpns comments was not accessible\n",
      "received 403 HTTP response\n",
      "2mj0hos comments was not accessible\n",
      "received 403 HTTP response\n",
      "2rr5ghs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2htu52s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2cosp6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2x89yos comments was not accessible\n",
      "received 403 HTTP response\n",
      "2a4ma1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2dsnjps comments was not accessible\n",
      "received 403 HTTP response\n",
      "2nqj8ds comments was not accessible\n",
      "received 403 HTTP response\n",
      "2vyx53s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2jhxgds comments was not accessible\n",
      "received 403 HTTP response\n",
      "2g5gkds comments was not accessible\n",
      "received 403 HTTP response\n",
      "2bpcl4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2j16qjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2l0oovs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2lrjtws comments was not accessible\n",
      "received 403 HTTP response\n",
      "2bvvdts comments was not accessible\n",
      "received 403 HTTP response\n",
      "2a9vovs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2k8s4ks comments was not accessible\n",
      "received 403 HTTP response\n",
      "2s448us comments was not accessible\n",
      "received 403 HTTP response\n",
      "1g0g2ks comments was not accessible\n",
      "received 403 HTTP response\n",
      "1m76pjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "1fri8qs comments was not accessible\n",
      "received 403 HTTP response\n",
      "hsiums comments was not accessible\n",
      "received 403 HTTP response\n",
      "g60w5s comments was not accessible\n",
      "received 403 HTTP response\n",
      "1ngt2es comments was not accessible\n",
      "received 403 HTTP response\n",
      "1l66yss comments was not accessible\n",
      "received 403 HTTP response\n",
      "1atoens comments was not accessible\n",
      "received 403 HTTP response\n",
      "1ghaprs comments was not accessible\n",
      "received 403 HTTP response\n",
      "toys9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2dmd9gs comments was not accessible\n",
      "received 403 HTTP response\n",
      "m0t93s comments was not accessible\n",
      "received 403 HTTP response\n",
      "l2rzfs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2x13hts comments was not accessible\n",
      "received 403 HTTP response\n",
      "2crxpbs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2jgqvys comments was not accessible\n",
      "received 403 HTTP response\n",
      "2vrioos comments was not accessible\n",
      "received 403 HTTP response\n",
      "2qx9bls comments was not accessible\n",
      "received 403 HTTP response\n",
      "2kjbnxs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2jziccs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2dcm4ds comments was not accessible\n",
      "received 403 HTTP response\n",
      "2vrcpns comments was not accessible\n",
      "received 403 HTTP response\n",
      "2okb8ss comments was not accessible\n",
      "received 403 HTTP response\n",
      "2ojya1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2kc9r3s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2jhxgds comments was not accessible\n",
      "received 403 HTTP response\n",
      "2ck9c7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2wiq8es comments was not accessible\n",
      "received 403 HTTP response\n",
      "2vdgp2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2rr5ghs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2nqj8ds comments was not accessible\n",
      "received 403 HTTP response\n",
      "2b7hq2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "29n6hxs comments was not accessible\n",
      "received 403 HTTP response\n",
      "29kg2vs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2wigh9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2py60ws comments was not accessible\n",
      "received 403 HTTP response\n",
      "2pxfo8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2ooqlbs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2of93js comments was not accessible\n",
      "received 403 HTTP response\n",
      "2jln5us comments was not accessible\n",
      "received 403 HTTP response\n",
      "2j16qjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2h501as comments was not accessible\n",
      "received 403 HTTP response\n",
      "2f5iups comments was not accessible\n",
      "received 403 HTTP response\n",
      "2wvxows comments was not accessible\n",
      "received 403 HTTP response\n",
      "2tka3qs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2owlzjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2j0b5ys comments was not accessible\n",
      "received 403 HTTP response\n",
      "2izslzs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2iujfas comments was not accessible\n",
      "received 403 HTTP response\n",
      "2gd1ols comments was not accessible\n",
      "received 403 HTTP response\n",
      "2dzi3ws comments was not accessible\n",
      "received 403 HTTP response\n",
      "2ckoois comments was not accessible\n",
      "received 403 HTTP response\n",
      "2e5og2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "733dlzs comments was not accessible\n",
      "received 403 HTTP response\n",
      "725kq6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "7invghs comments was not accessible\n",
      "received 403 HTTP response\n",
      "97v2y4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "830kbms comments was not accessible\n",
      "received 403 HTTP response\n",
      "7jzyl0s comments was not accessible\n",
      "received 403 HTTP response\n",
      "78pbd6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2dmdbas comments was not accessible\n",
      "received 403 HTTP response\n",
      "2gvh0xs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2zaimus comments was not accessible\n",
      "received 403 HTTP response\n",
      "1soj34s comments was not accessible\n",
      "received 403 HTTP response\n",
      "24w9i8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "kdrykqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "ikvejjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "qw8k0es comments was not accessible\n",
      "received 403 HTTP response\n",
      "l3kt4ns comments was not accessible\n",
      "received 403 HTTP response\n",
      "gj9lt8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "prjl8vs comments was not accessible\n",
      "received 403 HTTP response\n",
      "i5gyvos comments was not accessible\n",
      "received 403 HTTP response\n",
      "t4b9s4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "khymtys comments was not accessible\n",
      "received 403 HTTP response\n",
      "t17pivs comments was not accessible\n",
      "received 403 HTTP response\n",
      "p93j7rs comments was not accessible\n",
      "received 403 HTTP response\n",
      "j2fwsjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "olepias comments was not accessible\n",
      "received 403 HTTP response\n",
      "lsxpabs comments was not accessible\n",
      "received 403 HTTP response\n",
      "o0bu9gs comments was not accessible\n",
      "received 403 HTTP response\n",
      "t4lrats comments was not accessible\n",
      "received 403 HTTP response\n",
      "p68o85s comments was not accessible\n",
      "received 403 HTTP response\n",
      "k3bupjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "kvoce5s comments was not accessible\n",
      "received 403 HTTP response\n",
      "jgzxm8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "qwyrlts comments was not accessible\n",
      "received 403 HTTP response\n",
      "pvm9x9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "i68k87s comments was not accessible\n",
      "received 403 HTTP response\n",
      "o3znx2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "f0w74fs comments was not accessible\n",
      "received 403 HTTP response\n",
      "pq92e6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "dicxwps comments was not accessible\n",
      "received 403 HTTP response\n",
      "t4djyqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "q339fbs comments was not accessible\n",
      "received 403 HTTP response\n",
      "fmjk3rs comments was not accessible\n",
      "received 403 HTTP response\n",
      "fn2hjcs comments was not accessible\n",
      "received 403 HTTP response\n",
      "g11kmvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "fu75yhs comments was not accessible\n",
      "received 403 HTTP response\n",
      "fupxubs comments was not accessible\n",
      "received 403 HTTP response\n",
      "g0g0c7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "d5o2p8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "hgr8zis comments was not accessible\n",
      "received 403 HTTP response\n",
      "gj9lt8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "gv5e7ts comments was not accessible\n",
      "received 403 HTTP response\n",
      "hcj1prs comments was not accessible\n",
      "received 403 HTTP response\n",
      "ip3g04s comments was not accessible\n",
      "received 403 HTTP response\n",
      "pdcdnys comments was not accessible\n",
      "received 403 HTTP response\n",
      "pft2nds comments was not accessible\n",
      "received 403 HTTP response\n",
      "p8598ks comments was not accessible\n",
      "received 403 HTTP response\n",
      "gyngtps comments was not accessible\n",
      "received 403 HTTP response\n",
      "flwqrvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "c4x57zs comments was not accessible\n",
      "received 403 HTTP response\n",
      "gr5kqls comments was not accessible\n",
      "received 403 HTTP response\n",
      "gijdd8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "evt4m9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "78p4rns comments was not accessible\n",
      "received 403 HTTP response\n",
      "cua3j1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "hngm0ds comments was not accessible\n",
      "received 403 HTTP response\n",
      "h8vt5js comments was not accessible\n",
      "received 403 HTTP response\n",
      "2hj4dds comments was not accessible\n",
      "received 403 HTTP response\n",
      "2weyh8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2pt7ais comments was not accessible\n",
      "received 403 HTTP response\n",
      "2py9sws comments was not accessible\n",
      "received 403 HTTP response\n",
      "2ail6is comments was not accessible\n",
      "received 403 HTTP response\n",
      "2p08hps comments was not accessible\n",
      "received 403 HTTP response\n",
      "2x13hts comments was not accessible\n",
      "received 403 HTTP response\n",
      "2wiq8es comments was not accessible\n",
      "received 403 HTTP response\n",
      "2qoyo4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2ligpns comments was not accessible\n",
      "received 403 HTTP response\n",
      "2br8wss comments was not accessible\n",
      "received 403 HTTP response\n",
      "2tjjjcs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2k0rb9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "29xt98s comments was not accessible\n",
      "received 403 HTTP response\n",
      "29kaius comments was not accessible\n",
      "received 403 HTTP response\n",
      "2vrwfks comments was not accessible\n",
      "received 403 HTTP response\n",
      "2dcm4ds comments was not accessible\n",
      "received 403 HTTP response\n",
      "1m76pjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "1g0g2ks comments was not accessible\n",
      "received 403 HTTP response\n",
      "1i4mlds comments was not accessible\n",
      "received 403 HTTP response\n",
      "1kyx63s comments was not accessible\n",
      "received 403 HTTP response\n",
      "1222wms comments was not accessible\n",
      "received 403 HTTP response\n",
      "1xk6l4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "1atoens comments was not accessible\n",
      "received 403 HTTP response\n",
      "186kyus comments was not accessible\n",
      "received 403 HTTP response\n",
      "11h7was comments was not accessible\n",
      "received 403 HTTP response\n",
      "16cbyms comments was not accessible\n",
      "received 403 HTTP response\n",
      "2dmdbas comments was not accessible\n",
      "received 403 HTTP response\n",
      "1ua2uqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "1p8ia9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "1ngt2es comments was not accessible\n",
      "received 403 HTTP response\n",
      "1h7hcps comments was not accessible\n",
      "received 403 HTTP response\n",
      "o61at6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "d5o2p8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "m7yn70s comments was not accessible\n",
      "received 403 HTTP response\n",
      "lnh4p9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "gj9lt8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "o7yw6xs comments was not accessible\n",
      "received 403 HTTP response\n",
      "hcj2xys comments was not accessible\n",
      "received 403 HTTP response\n",
      "tic7sls comments was not accessible\n",
      "received 403 HTTP response\n",
      "fqkv5ns comments was not accessible\n",
      "received 403 HTTP response\n",
      "gv5e7ts comments was not accessible\n",
      "received 403 HTTP response\n",
      "pn07qds comments was not accessible\n",
      "received 403 HTTP response\n",
      "fti54ds comments was not accessible\n",
      "received 403 HTTP response\n",
      "2x89yos comments was not accessible\n",
      "received 403 HTTP response\n",
      "2vhdivs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2tju8hs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2sbb50s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2sn7xcs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6oz1aps comments was not accessible\n",
      "received 403 HTTP response\n",
      "8ustwfs comments was not accessible\n",
      "received 403 HTTP response\n",
      "733dlzs comments was not accessible\n",
      "received 403 HTTP response\n",
      "8xp9sus comments was not accessible\n",
      "received 403 HTTP response\n",
      "7pzgm5s comments was not accessible\n",
      "received 403 HTTP response\n",
      "7invghs comments was not accessible\n",
      "received 403 HTTP response\n",
      "71lbyjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "7nahxls comments was not accessible\n",
      "received 403 HTTP response\n",
      "8kwt4ls comments was not accessible\n",
      "received 403 HTTP response\n",
      "8kuhl2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "8cjs4ks comments was not accessible\n",
      "received 403 HTTP response\n",
      "7w0nr7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "7ayvzgs comments was not accessible\n",
      "received 403 HTTP response\n",
      "75ys62s comments was not accessible\n",
      "received 403 HTTP response\n",
      "6vy001s comments was not accessible\n",
      "received 403 HTTP response\n",
      "8ivkb2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "6z2ihes comments was not accessible\n",
      "received 403 HTTP response\n",
      "6tprebs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2hj4dds comments was not accessible\n",
      "received 403 HTTP response\n",
      "2ligpns comments was not accessible\n",
      "received 403 HTTP response\n",
      "2x13hts comments was not accessible\n",
      "received 403 HTTP response\n",
      "2pgd1os comments was not accessible\n",
      "received 403 HTTP response\n",
      "2a4ma1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2wiq8es comments was not accessible\n",
      "received 403 HTTP response\n",
      "2ne6n4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2az986s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2wyu08s comments was not accessible\n",
      "received 403 HTTP response\n",
      "2qbn7ps comments was not accessible\n",
      "received 403 HTTP response\n",
      "2x89yos comments was not accessible\n",
      "received 403 HTTP response\n",
      "2u08p4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "1g0g2ks comments was not accessible\n",
      "received 403 HTTP response\n",
      "1m76pjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "1i4mlds comments was not accessible\n",
      "received 403 HTTP response\n",
      "1fri8qs comments was not accessible\n",
      "received 403 HTTP response\n",
      "1fvtzis comments was not accessible\n",
      "received 403 HTTP response\n",
      "1s4n48s comments was not accessible\n",
      "received 403 HTTP response\n",
      "1222wms comments was not accessible\n",
      "received 403 HTTP response\n",
      "1rgpvhs comments was not accessible\n",
      "received 403 HTTP response\n",
      "1p5aoes comments was not accessible\n",
      "received 403 HTTP response\n",
      "nuq6uws comments was not accessible\n",
      "received 403 HTTP response\n",
      "iw7b2cs comments was not accessible\n",
      "received 403 HTTP response\n",
      "jgysu0s comments was not accessible\n",
      "received 403 HTTP response\n",
      "gxhgp5s comments was not accessible\n",
      "received 403 HTTP response\n",
      "n3plmis comments was not accessible\n",
      "received 403 HTTP response\n",
      "lomgu5s comments was not accessible\n",
      "received 403 HTTP response\n",
      "mhmrfds comments was not accessible\n",
      "received 403 HTTP response\n",
      "o7yw6xs comments was not accessible\n",
      "received 403 HTTP response\n",
      "q55hebs comments was not accessible\n",
      "received 403 HTTP response\n",
      "pwbh47s comments was not accessible\n",
      "received 403 HTTP response\n",
      "f4zrecs comments was not accessible\n",
      "received 403 HTTP response\n",
      "azjrzus comments was not accessible\n",
      "received 403 HTTP response\n",
      "es0n7os comments was not accessible\n",
      "received 403 HTTP response\n",
      "amrrt8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "awxx0as comments was not accessible\n",
      "received 403 HTTP response\n",
      "b0r7wms comments was not accessible\n",
      "received 403 HTTP response\n",
      "7invghs comments was not accessible\n",
      "received 403 HTTP response\n",
      "7pzgm5s comments was not accessible\n",
      "received 403 HTTP response\n",
      "flwqrvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "ez1fsys comments was not accessible\n",
      "received 403 HTTP response\n",
      "ele4s6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "d1dhhls comments was not accessible\n",
      "received 403 HTTP response\n",
      "aq6ciqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "ife2q4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "fe03w8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "733dlzs comments was not accessible\n",
      "received 403 HTTP response\n",
      "pc0hrvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "a72ae4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "8xp9sus comments was not accessible\n",
      "received 403 HTTP response\n",
      "8ustwfs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6vy001s comments was not accessible\n",
      "received 403 HTTP response\n",
      "kjw4rcs comments was not accessible\n",
      "received 403 HTTP response\n",
      "ja8b9xs comments was not accessible\n",
      "received 403 HTTP response\n",
      "gnxpqvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "f3i67is comments was not accessible\n",
      "received 403 HTTP response\n",
      "e913o2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "a1nf8xs comments was not accessible\n",
      "received 403 HTTP response\n",
      "8h1p8zs comments was not accessible\n",
      "received 403 HTTP response\n",
      "71lbyjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "h7ll2ws comments was not accessible\n",
      "received 403 HTTP response\n",
      "eha9lcs comments was not accessible\n",
      "received 403 HTTP response\n",
      "bdtvxzs comments was not accessible\n",
      "received 403 HTTP response\n",
      "bcd3urs comments was not accessible\n",
      "received 403 HTTP response\n",
      "aoxnsqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "a4wgq7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "8cjs4ks comments was not accessible\n",
      "received 403 HTTP response\n",
      "7nahxls comments was not accessible\n",
      "received 403 HTTP response\n",
      "f2vq91s comments was not accessible\n",
      "received 403 HTTP response\n",
      "besc66s comments was not accessible\n",
      "received 403 HTTP response\n",
      "t9e9tss comments was not accessible\n",
      "received 403 HTTP response\n",
      "lhq86fs comments was not accessible\n",
      "received 403 HTTP response\n",
      "t5qufjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "nnmtngs comments was not accessible\n",
      "received 403 HTTP response\n",
      "t88wots comments was not accessible\n",
      "received 403 HTTP response\n",
      "7bcyh2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "75v2aks comments was not accessible\n",
      "received 403 HTTP response\n",
      "71qnp4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "8vm4aos comments was not accessible\n",
      "received 403 HTTP response\n",
      "8zkdyks comments was not accessible\n",
      "received 403 HTTP response\n",
      "8tifejs comments was not accessible\n",
      "received 403 HTTP response\n",
      "7hxmd8s comments was not accessible\n",
      "received 500 HTTP response\n",
      "85bc74s comments was not accessible\n",
      "received 500 HTTP response\n",
      "8s27yds comments was not accessible\n",
      "received 500 HTTP response\n",
      "bontoas comments was not accessible\n",
      "received 500 HTTP response\n",
      "jesodts comments was not accessible\n",
      "received 500 HTTP response\n",
      "enf0jis comments was not accessible\n",
      "received 500 HTTP response\n",
      "ekhua7s comments was not accessible\n",
      "received 500 HTTP response\n",
      "bkd37is comments was not accessible\n",
      "received 500 HTTP response\n",
      "mso29ts comments was not accessible\n",
      "received 500 HTTP response\n",
      "ghlhm4s comments was not accessible\n",
      "received 500 HTTP response\n",
      "ibbvc2s comments was not accessible\n",
      "received 500 HTTP response\n",
      "ap80t7s comments was not accessible\n",
      "received 500 HTTP response\n",
      "nkwmrts comments was not accessible\n",
      "received 500 HTTP response\n",
      "jkia1as comments was not accessible\n",
      "received 500 HTTP response\n",
      "hjfmfjs comments was not accessible\n",
      "received 500 HTTP response\n",
      "c6jcats comments was not accessible\n",
      "received 500 HTTP response\n",
      "azo8r8s comments was not accessible\n",
      "received 500 HTTP response\n",
      "bmtluhs comments was not accessible\n",
      "received 500 HTTP response\n",
      "nmw41ds comments was not accessible\n",
      "received 500 HTTP response\n",
      "lp2lo5s comments was not accessible\n",
      "received 500 HTTP response\n",
      "s1q8mgs comments was not accessible\n",
      "received 500 HTTP response\n",
      "kp1f1rs comments was not accessible\n",
      "received 500 HTTP response\n",
      "s5audss comments was not accessible\n",
      "received 500 HTTP response\n",
      "lszc8fs comments was not accessible\n",
      "received 500 HTTP response\n",
      "t1of50s comments was not accessible\n",
      "received 500 HTTP response\n",
      "ohl1m5s comments was not accessible\n",
      "received 500 HTTP response\n",
      "qj1ctqs comments was not accessible\n",
      "received 500 HTTP response\n",
      "4yfb5ks comments was not accessible\n",
      "received 500 HTTP response\n",
      "4pgtr5s comments was not accessible\n",
      "received 500 HTTP response\n",
      "52wdi1s comments was not accessible\n",
      "received 500 HTTP response\n",
      "50tb8fs comments was not accessible\n",
      "received 500 HTTP response\n",
      "4x8k4ms comments was not accessible\n",
      "received 500 HTTP response\n",
      "5g5adds comments was not accessible\n",
      "received 500 HTTP response\n",
      "5f39f9s comments was not accessible\n",
      "received 500 HTTP response\n",
      "4pqvfgs comments was not accessible\n",
      "received 500 HTTP response\n",
      "cebo91s comments was not accessible\n",
      "received 500 HTTP response\n",
      "5cyyvfs comments was not accessible\n",
      "received 500 HTTP response\n",
      "4r8kh7s comments was not accessible\n",
      "received 500 HTTP response\n",
      "5f0j0qs comments was not accessible\n",
      "received 500 HTTP response\n",
      "gghh4cs comments was not accessible\n",
      "received 500 HTTP response\n",
      "jrzrnds comments was not accessible\n",
      "received 500 HTTP response\n",
      "jfjv32s comments was not accessible\n",
      "received 500 HTTP response\n",
      "n2f69ys comments was not accessible\n",
      "received 500 HTTP response\n",
      "kv0u45s comments was not accessible\n",
      "received 500 HTTP response\n",
      "jpflg0s comments was not accessible\n",
      "received 500 HTTP response\n",
      "kpbp1ys comments was not accessible\n",
      "received 500 HTTP response\n",
      "k2y3m0s comments was not accessible\n",
      "received 500 HTTP response\n",
      "m5uecys comments was not accessible\n",
      "received 500 HTTP response\n",
      "n3xf92s comments was not accessible\n",
      "received 500 HTTP response\n",
      "k7feo4s comments was not accessible\n",
      "received 500 HTTP response\n",
      "itfusis comments was not accessible\n",
      "received 500 HTTP response\n",
      "ipp455s comments was not accessible\n",
      "received 500 HTTP response\n",
      "l5i21cs comments was not accessible\n",
      "received 500 HTTP response\n",
      "k804svs comments was not accessible\n",
      "received 500 HTTP response\n",
      "jfjse0s comments was not accessible\n",
      "received 500 HTTP response\n",
      "nlo1xms comments was not accessible\n",
      "received 500 HTTP response\n",
      "nul60vs comments was not accessible\n",
      "received 500 HTTP response\n",
      "leuqyes comments was not accessible\n",
      "received 500 HTTP response\n",
      "iskhmjs comments was not accessible\n",
      "received 500 HTTP response\n",
      "lmusx3s comments was not accessible\n",
      "received 500 HTTP response\n",
      "l8rmqis comments was not accessible\n",
      "received 500 HTTP response\n",
      "is44k4s comments was not accessible\n",
      "received 500 HTTP response\n",
      "kh1a8ms comments was not accessible\n",
      "received 500 HTTP response\n",
      "kdox3ss comments was not accessible\n",
      "received 500 HTTP response\n",
      "l9gj25s comments was not accessible\n",
      "received 500 HTTP response\n",
      "iiuqnos comments was not accessible\n",
      "received 500 HTTP response\n",
      "lhpw7ms comments was not accessible\n",
      "received 500 HTTP response\n",
      "iitl0hs comments was not accessible\n",
      "received 500 HTTP response\n",
      "lx42n1s comments was not accessible\n",
      "received 500 HTTP response\n",
      "luu9ges comments was not accessible\n",
      "received 500 HTTP response\n",
      "iqxnqxs comments was not accessible\n",
      "received 500 HTTP response\n",
      "lx3qrss comments was not accessible\n",
      "received 500 HTTP response\n",
      "mx3sngs comments was not accessible\n",
      "received 500 HTTP response\n",
      "mzzelfs comments was not accessible\n",
      "received 500 HTTP response\n",
      "j0vgsrs comments was not accessible\n",
      "received 500 HTTP response\n",
      "kv0v0es comments was not accessible\n",
      "received 500 HTTP response\n",
      "lkjcpws comments was not accessible\n",
      "received 500 HTTP response\n",
      "n3xms7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "kjw4rcs comments was not accessible\n",
      "received 403 HTTP response\n",
      "l3f7i6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "nvm9txs comments was not accessible\n",
      "received 403 HTTP response\n",
      "m7gx2as comments was not accessible\n",
      "received 403 HTTP response\n",
      "nvm9txs comments was not accessible\n",
      "received 403 HTTP response\n",
      "l3f7i6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "jemix0s comments was not accessible\n",
      "received 403 HTTP response\n",
      "hvooe6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "k7r9jls comments was not accessible\n",
      "received 403 HTTP response\n",
      "i1sjxus comments was not accessible\n",
      "received 403 HTTP response\n",
      "k9kdtns comments was not accessible\n",
      "received 403 HTTP response\n",
      "ifhg6ts comments was not accessible\n",
      "received 403 HTTP response\n",
      "pijlcrs comments was not accessible\n",
      "received 403 HTTP response\n",
      "t3exubs comments was not accessible\n",
      "received 403 HTTP response\n",
      "jskij4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "kf77uhs comments was not accessible\n",
      "received 403 HTTP response\n",
      "nhts94s comments was not accessible\n",
      "received 403 HTTP response\n",
      "niisg1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "ksiyo1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "jfqh4hs comments was not accessible\n",
      "received 403 HTTP response\n",
      "lk5z7hs comments was not accessible\n",
      "received 403 HTTP response\n",
      "l8puy6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "k5siwhs comments was not accessible\n",
      "received 403 HTTP response\n",
      "m39tjms comments was not accessible\n",
      "received 403 HTTP response\n",
      "s1a8dvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "o5y2j2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "nce02cs comments was not accessible\n",
      "received 403 HTTP response\n",
      "kuwazys comments was not accessible\n",
      "received 403 HTTP response\n",
      "ssvvc9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "mex91us comments was not accessible\n",
      "received 403 HTTP response\n",
      "nb2lpvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "s86scms comments was not accessible\n",
      "received 403 HTTP response\n",
      "mymutrs comments was not accessible\n",
      "received 403 HTTP response\n",
      "gfn34ls comments was not accessible\n",
      "received 403 HTTP response\n",
      "m4t1trs comments was not accessible\n",
      "received 403 HTTP response\n",
      "7tefkcs comments was not accessible\n",
      "received 403 HTTP response\n",
      "h7q5xis comments was not accessible\n",
      "received 403 HTTP response\n",
      "ad82e8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "oh78n6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "lam06ds comments was not accessible\n",
      "received 403 HTTP response\n",
      "kv76acs comments was not accessible\n",
      "received 403 HTTP response\n",
      "dynhbvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "d51ie8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "bhjqkss comments was not accessible\n",
      "received 403 HTTP response\n",
      "b1lkbcs comments was not accessible\n",
      "received 403 HTTP response\n",
      "iczu7qs comments was not accessible\n",
      "received 403 HTTP response\n",
      "hso2ots comments was not accessible\n",
      "received 403 HTTP response\n",
      "gfmh27s comments was not accessible\n",
      "received 403 HTTP response\n",
      "bxelils comments was not accessible\n",
      "received 403 HTTP response\n",
      "mclu2bs comments was not accessible\n",
      "received 403 HTTP response\n",
      "mo5gr5s comments was not accessible\n",
      "received 403 HTTP response\n",
      "ly1hxos comments was not accessible\n",
      "received 403 HTTP response\n",
      "l49f3bs comments was not accessible\n",
      "received 403 HTTP response\n",
      "j5ub9ls comments was not accessible\n",
      "received 403 HTTP response\n",
      "iqwvwrs comments was not accessible\n",
      "received 403 HTTP response\n",
      "hho0czs comments was not accessible\n",
      "received 403 HTTP response\n",
      "gyg8ous comments was not accessible\n",
      "received 403 HTTP response\n",
      "grz8bvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "b1lkbcs comments was not accessible\n",
      "received 403 HTTP response\n",
      "enx0t0s comments was not accessible\n",
      "received 403 HTTP response\n",
      "dp4xfks comments was not accessible\n",
      "received 403 HTTP response\n",
      "ccfsq2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "c48jeas comments was not accessible\n",
      "received 403 HTTP response\n",
      "c3hjnms comments was not accessible\n",
      "received 403 HTTP response\n",
      "c0y24is comments was not accessible\n",
      "received 403 HTTP response\n",
      "bt0a23s comments was not accessible\n",
      "received 403 HTTP response\n",
      "7407u4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "qjqbias comments was not accessible\n",
      "received 403 HTTP response\n",
      "naesw6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "tbjae0s comments was not accessible\n",
      "received 403 HTTP response\n",
      "t2dhdds comments was not accessible\n",
      "received 403 HTTP response\n",
      "lhq86fs comments was not accessible\n",
      "received 403 HTTP response\n",
      "m6gaygs comments was not accessible\n",
      "received 403 HTTP response\n",
      "ta09ras comments was not accessible\n",
      "received 403 HTTP response\n",
      "q8ssy4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "qvwcy7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "nti8sts comments was not accessible\n",
      "received 403 HTTP response\n",
      "squv0vs comments was not accessible\n",
      "received 403 HTTP response\n",
      "lgd7zrs comments was not accessible\n",
      "received 403 HTTP response\n",
      "kxubwcs comments was not accessible\n",
      "received 403 HTTP response\n",
      "p41bdzs comments was not accessible\n",
      "received 403 HTTP response\n",
      "os1piks comments was not accessible\n",
      "received 403 HTTP response\n",
      "m6ywyes comments was not accessible\n",
      "received 403 HTTP response\n",
      "khymtys comments was not accessible\n",
      "received 403 HTTP response\n",
      "mt1wz1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "q339fbs comments was not accessible\n",
      "received 403 HTTP response\n",
      "oxzkt1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "m00t9ts comments was not accessible\n",
      "received 403 HTTP response\n",
      "pndl5ls comments was not accessible\n",
      "received 403 HTTP response\n",
      "n05wpos comments was not accessible\n",
      "received 403 HTTP response\n",
      "shxbh4s comments was not accessible\n",
      "received 403 HTTP response\n",
      "rfq1ufs comments was not accessible\n",
      "received 403 HTTP response\n",
      "q3pw0ls comments was not accessible\n",
      "received 403 HTTP response\n",
      "qkjydts comments was not accessible\n",
      "received 403 HTTP response\n",
      "q8vltss comments was not accessible\n",
      "received 403 HTTP response\n",
      "pq92e6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "phnasys comments was not accessible\n",
      "received 403 HTTP response\n",
      "mtiatls comments was not accessible\n",
      "received 403 HTTP response\n",
      "lo95i6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "jg3qwvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "qym6ios comments was not accessible\n",
      "received 403 HTTP response\n",
      "qj96tss comments was not accessible\n",
      "received 403 HTTP response\n",
      "ldbmg5s comments was not accessible\n",
      "received 403 HTTP response\n",
      "t4djyqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "otpggts comments was not accessible\n",
      "received 403 HTTP response\n",
      "r6kv1ts comments was not accessible\n",
      "received 403 HTTP response\n",
      "qo5z1bs comments was not accessible\n",
      "received 403 HTTP response\n",
      "rlqgsrs comments was not accessible\n",
      "received 403 HTTP response\n",
      "qtx4yps comments was not accessible\n",
      "received 403 HTTP response\n",
      "qpk4xxs comments was not accessible\n",
      "received 403 HTTP response\n",
      "rvtst2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "r57lbks comments was not accessible\n",
      "received 403 HTTP response\n",
      "r14h1js comments was not accessible\n",
      "received 403 HTTP response\n",
      "radcljs comments was not accessible\n",
      "received 403 HTTP response\n",
      "pvm9x9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "tk7vkbs comments was not accessible\n",
      "received 403 HTTP response\n",
      "tcindgs comments was not accessible\n",
      "received 403 HTTP response\n",
      "rxtm37s comments was not accessible\n",
      "received 403 HTTP response\n",
      "pp949hs comments was not accessible\n",
      "received 403 HTTP response\n",
      "pni14xs comments was not accessible\n",
      "received 403 HTTP response\n",
      "su2nlds comments was not accessible\n",
      "received 403 HTTP response\n",
      "rgjudus comments was not accessible\n",
      "received 403 HTTP response\n",
      "qt6qqqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "qtgsevs comments was not accessible\n",
      "received 403 HTTP response\n",
      "ehuisqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6zx5vas comments was not accessible\n",
      "received 403 HTTP response\n",
      "6vy001s comments was not accessible\n",
      "received 403 HTTP response\n",
      "bl550os comments was not accessible\n",
      "received 403 HTTP response\n",
      "d94t8js comments was not accessible\n",
      "received 403 HTTP response\n",
      "bg73p5s comments was not accessible\n",
      "received 403 HTTP response\n",
      "m6wrj1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "hovsk7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "ele4s6s comments was not accessible\n",
      "received 403 HTTP response\n",
      "6vpw5gs comments was not accessible\n",
      "received 403 HTTP response\n",
      "evqui7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "aztscrs comments was not accessible\n",
      "received 403 HTTP response\n",
      "bedqnks comments was not accessible\n",
      "received 403 HTTP response\n",
      "an3hcus comments was not accessible\n",
      "received 403 HTTP response\n",
      "8zjyb5s comments was not accessible\n",
      "received 403 HTTP response\n",
      "6x1njms comments was not accessible\n",
      "received 403 HTTP response\n",
      "6q7atvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6py4nos comments was not accessible\n",
      "received 403 HTTP response\n",
      "6pf3tls comments was not accessible\n",
      "received 403 HTTP response\n",
      "pcalams comments was not accessible\n",
      "received 403 HTTP response\n",
      "t700w2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "p41bdzs comments was not accessible\n",
      "received 403 HTTP response\n",
      "pvm9x9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "pibbz0s comments was not accessible\n",
      "received 403 HTTP response\n",
      "rwpb30s comments was not accessible\n",
      "received 403 HTTP response\n",
      "qqdtaks comments was not accessible\n",
      "received 403 HTTP response\n",
      "nuq6uws comments was not accessible\n",
      "received 403 HTTP response\n",
      "ln56c2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "rd0u20s comments was not accessible\n",
      "received 403 HTTP response\n",
      "n45lzes comments was not accessible\n",
      "received 403 HTTP response\n",
      "i5gyvos comments was not accessible\n",
      "received 403 HTTP response\n",
      "pvm9x9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "po2yxms comments was not accessible\n",
      "received 403 HTTP response\n",
      "ldymmus comments was not accessible\n",
      "received 403 HTTP response\n",
      "iib4c7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "sqiuz8s comments was not accessible\n",
      "received 403 HTTP response\n",
      "mde6kes comments was not accessible\n",
      "received 403 HTTP response\n",
      "i5gyvos comments was not accessible\n",
      "received 403 HTTP response\n",
      "p41bdzs comments was not accessible\n",
      "received 403 HTTP response\n",
      "iib4c7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "rqpqqks comments was not accessible\n",
      "received 403 HTTP response\n",
      "nkxsw2s comments was not accessible\n",
      "received 403 HTTP response\n",
      "l8w04ss comments was not accessible\n",
      "received 403 HTTP response\n",
      "rw2g73s comments was not accessible\n",
      "received 403 HTTP response\n",
      "rxtm37s comments was not accessible\n",
      "received 403 HTTP response\n",
      "qxq9fus comments was not accessible\n",
      "received 403 HTTP response\n",
      "oux3k9s comments was not accessible\n",
      "received 403 HTTP response\n",
      "iib4c7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "szq522s comments was not accessible\n",
      "received 403 HTTP response\n",
      "s86scms comments was not accessible\n",
      "results/subnetworks\\left.csv  is finished\n",
      "Opening results/subnetworks\\meta_reddit.csv\n",
      "received 403 HTTP response\n",
      "39hyors comments was not accessible\n",
      "received 403 HTTP response\n",
      "39hyors comments was not accessible\n",
      "received 403 HTTP response\n",
      "39hyors comments was not accessible\n",
      "received 403 HTTP response\n",
      "39hyors comments was not accessible\n",
      "received 403 HTTP response\n",
      "39hyors comments was not accessible\n",
      "received 403 HTTP response\n",
      "39hyors comments was not accessible\n",
      "received 403 HTTP response\n",
      "39hyors comments was not accessible\n",
      "received 403 HTTP response\n",
      "39hyors comments was not accessible\n",
      "received 403 HTTP response\n",
      "3ceysqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "3ceysqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "3ceysqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "3ceysqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "3ceysqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "3ceysqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "3ceysqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "3ceysqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "i7uo9as comments was not accessible\n",
      "received 403 HTTP response\n",
      "i7wvk7s comments was not accessible\n",
      "received 403 HTTP response\n",
      "cryghvs comments was not accessible\n",
      "received 403 HTTP response\n",
      "c68ooqs comments was not accessible\n",
      "received 403 HTTP response\n",
      "2zatbss comments was not accessible\n",
      "received 403 HTTP response\n",
      "6maiyhs comments was not accessible\n",
      "received 403 HTTP response\n",
      "71bi2bs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6qwo1ds comments was not accessible\n",
      "received 403 HTTP response\n",
      "6kmpmcs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6lz9rss comments was not accessible\n",
      "received 403 HTTP response\n",
      "6s6jvjs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6iw7jks comments was not accessible\n",
      "received 403 HTTP response\n",
      "6b1egfs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6teakis comments was not accessible\n",
      "received 403 HTTP response\n",
      "6nkpkrs comments was not accessible\n",
      "received 403 HTTP response\n",
      "68t68zs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6kzgits comments was not accessible\n",
      "received 403 HTTP response\n",
      "6jwlr1s comments was not accessible\n",
      "received 403 HTTP response\n",
      "6ne15ys comments was not accessible\n",
      "received 403 HTTP response\n",
      "6qi9vys comments was not accessible\n",
      "received 403 HTTP response\n",
      "6qk2l5s comments was not accessible\n",
      "received 403 HTTP response\n",
      "6k0htgs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6mkkmbs comments was not accessible\n",
      "received 403 HTTP response\n",
      "6hnc7ks comments was not accessible\n",
      "received 403 HTTP response\n",
      "78pd6ys comments was not accessible\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "to_scan= [file for file in get_all_in_dir(\"results/subnetworks\")]\n",
    "for subreddit in to_scan:\n",
    "        comment_dict = dict()\n",
    "        print('Opening', subreddit)\n",
    "        try: \n",
    "\n",
    "                df = pd.read_csv(subreddit, encoding='utf8', on_bad_lines='skip')\n",
    "                \n",
    "                if os.path.isfile('comments_'+subreddit.split('\\\\')[1]):\n",
    "                        start = (pd.read_csv('comments_'+subreddit.split('\\\\')[1])['Source'].iloc[-1], pd.read_csv('comments_'+subreddit.split('\\\\')[1])['Target'].iloc[-1])\n",
    "                        start= int(df.loc[(df['Source'] == start[0]) & (df['Target'] == start[1])].index[0])\n",
    "                        if start >= len(df):\n",
    "                                continue        \n",
    "                        df = df[start+1:].dropna()\n",
    "        except Exception as e:\n",
    "               print('error',e,subreddit)\n",
    "               continue\n",
    "                \n",
    "        for row in df.to_dict(orient='records'):\n",
    "                if isinstance(row['Source'], float) or isinstance(row['Target'],float):\n",
    "                        continue\n",
    "                if 'https' in row['Source'] or 'https' in row['Target']:\n",
    "                        continue\n",
    "                try:\n",
    "                        list_post= row['Posts'][1:-1].split(',')\n",
    "                except Exception as e:\n",
    "                        continue\n",
    "                result = ''\n",
    "                num = 0\n",
    "                for post_url in list_post:\n",
    "                        if post_url == 0:\n",
    "                                continue\n",
    "                        post_url = post_url.replace(' ','')\n",
    "                        #tmp =  re.sub(\"\\[\\]\\'\", \"\", post_url) NON FUNZIONA E NON CAPISCO PERCHé\n",
    "                        if post_url[1:-1] in comment_dict.keys():\n",
    "                                result += comment_dict[post_url[1:-1]][0]\n",
    "                                num += comment_dict[post_url[1:-1]][1]\n",
    "                                continue\n",
    "                        try:\n",
    "                                post = reddit.submission(url=post_url[1:-1])\n",
    "                                try:\n",
    "                                        #This allows up to 1 reply to each post.\n",
    "                                        post.comments.replace_more(limit=1)\n",
    "                                except Exception as e:\n",
    "                                        print(e)\n",
    "                                        print(f'{post}s comments was not accessible')\n",
    "                                        continue\n",
    "                                num += len(post.comments)\n",
    "                                this_comment = ''\n",
    "                                if num > 500:\n",
    "                                        for comment in iterSample(post.comments, 500):\n",
    "                                                try:\n",
    "                                                        this_comment += comment.body\n",
    "                                                        for reply in comment.replies:\n",
    "                                                                this_comment+= reply.body\n",
    "                                                        this_comment=this_comment.replace(',','')\n",
    "                                                        this_comment=this_comment.replace('\\n','\\s')\n",
    "                                                except Exception as e:\n",
    "                                                        print(e)\n",
    "                                                        continue\n",
    "                                else:\n",
    "                                        for comment in post.comments:\n",
    "                                                try:\n",
    "                                                        this_comment += comment.body\n",
    "                                                        for reply in comment.replies:\n",
    "                                                                this_comment+= reply.body\n",
    "                                                        this_comment=this_comment.replace(',','')\n",
    "                                                        this_comment=this_comment.replace('\\n','\\s')\n",
    "\n",
    "                                                except Exception as e:\n",
    "                                                        print(e)\n",
    "                                                        continue\n",
    "                                comment_dict[post_url[1:-1]] = (this_comment,num)\n",
    "                                result += this_comment\n",
    "                        except Exception as e:\n",
    "                                print(e)\n",
    "                                print(post.permalink + ' is not accessible')\n",
    "                                continue                                         \n",
    "                res=pd.DataFrame.from_dict([{'Source':row['Source'], 'Target':row['Target'],'NumberComments': num,'Comments':result}])\n",
    "                if os.path.isfile('comments_'+subreddit.split('\\\\')[1]):\n",
    "                        res.to_csv('comments_'+subreddit.split('\\\\')[1],  encoding='utf8', mode='a', header=False)\n",
    "                else:\n",
    "                        res.to_csv('comments_'+subreddit.split('\\\\')[1],  encoding='utf8')\n",
    "                        \n",
    "        print(subreddit, ' is finished')\n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform the topic modeling: first we need to define the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print a csv for each crossposting between subreddits with the topic modeling of the common posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scan = [] #inserire iterazione su tutti i csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16812/4043230009.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mto_scan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Target'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Source'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mheat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "for file in to_scan:\n",
    "    df = pd.read_csv(file, encoding='utf8').sort_values('Target')\n",
    "    length = len(df)\n",
    "    df = df(index=[i for i in range(length)])\n",
    "    index = [df['Source']]\n",
    "    heat = pd.DataFrame(index = index)\n",
    "    targets = set(df['Target'])\n",
    "    for column in targets:\n",
    "        to_add = []\n",
    "        with_target=df[df['Target']==column]\n",
    "        start = df[df['Target']==column].first_valid_index()\n",
    "        end = df[df['Target']==column].last_valid_index()\n",
    "        if start > 0:\n",
    "            to_add=['NaN' for i in range(start)]\n",
    "            for row in with_target.to_dict(orient='records'):\n",
    "                to_add.append(row['Comments'])\n",
    "        else:\n",
    "            for row in with_target.to_dict(orient='records'):\n",
    "                to_add.append(row['Comments'])\n",
    "        if end < length-1:\n",
    "            to_add.extend('NaN' for i in range(length-1-end))\n",
    "        print(end)\n",
    "        heat[column] = to_add\n",
    "    df.to_csv(file.split('\\\\')[0]+'/heat_/'+file.split('\\\\'[1]), encoding='utf8')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor for\n",
    "for file in to_scan:\n",
    "    df = pd.read_csv(file, encoding='utf8')\n",
    "    origin, id = key.split('@ ')\n",
    "    if '1st_level_2' in origin:\n",
    "        origin = origin.split('1st_level_2')[1]\n",
    "        origin = origin.replace('\\\\','')\n",
    "        origin = origin.replace('.csv','')\n",
    "    if '1st_level' in origin:\n",
    "        origin = origin.split('1st_level')[1]\n",
    "        origin = origin.replace('\\\\','')\n",
    "        origin = origin.replace('.csv','')\n",
    "    if '2nd_level' in origin:\n",
    "        origin = origin.split('2nd_level')[1]\n",
    "        origin = origin.replace('\\\\','')\n",
    "        origin = origin.replace('.csv','')\n",
    "\n",
    "    id=id[id.index('/r/')+3:]\n",
    "    if 'x0' in key:\n",
    "        key = key.replace('x0','')\n",
    "\n",
    "    # Convert to list\n",
    "    data = []\n",
    "    mt = top_comments[key]\n",
    "    if len(mt) > 4:\n",
    "        for i in mt:\n",
    "            data.append(reddit.comment(i))\n",
    "        # Remove tags from comments\n",
    "        try:\n",
    "            data = [re.sub('\\S*@\\S*\\s?', '', sent.body) for sent in data]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        # Remove new line characters\n",
    "        data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "        # Remove distracting single quotes\n",
    "        data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "        # Tokenizing the text\n",
    "        data_words = list(sent_to_words(data))\n",
    "\n",
    "        # Build the bigram and trigram models\n",
    "        bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "        trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "        # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "        bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "        trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "        # Remove Stop Words\n",
    "        data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "        # Form Bigrams\n",
    "        data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "        # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "        # python3 -m spacy download en\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "        # Do lemmatization keeping only noun, adj, vb, adv\n",
    "        data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "        # Create Dictionary\n",
    "        id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "        # Create Corpus\n",
    "        texts = data_lemmatized\n",
    "\n",
    "        # Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        # Human readable format of corpus (term-frequency)\n",
    "        [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "        if corpus != [] and corpus != [[]]:\n",
    "            # Build LDA model\n",
    "            lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                    id2word=id2word,\n",
    "                                                    num_topics=10, \n",
    "                                                    random_state=100,\n",
    "                                                    update_every=1,\n",
    "                                                    chunksize=100,\n",
    "                                                    passes=10,\n",
    "                                                    alpha='auto',\n",
    "                                                    per_word_topics=True)\n",
    "\n",
    "            # Print the Keyword in the 10 topics\n",
    "            \n",
    "            index = []\n",
    "            topics = {}\n",
    "            list_topics=lda_model.print_topics()\n",
    "            for i in list_topics:\n",
    "                for value in range(len(list_topics)):\n",
    "                    to_add = []\n",
    "                    for el in list_topics[value][1].split('+'):\n",
    "                        start = el.index('\"')\n",
    "                        word = el[start+1:-2]             \n",
    "                        to_add.append(word)\n",
    "                    topics[value] = to_add\n",
    "\n",
    "            \n",
    "            df_topic = pd.DataFrame(topics)\n",
    "            done = pd.read_csv('done.csv', encoding='utf8')\n",
    "            done.insert(0,key,'NaN')\n",
    "            done.to_csv('done.csv' ,encoding='utf8', mode='w')\n",
    "\n",
    "            for row in df_topic.index:\n",
    "                index.append(origin)\n",
    "            df_topic['origin'] = index\n",
    "            if os.path.isfile(r\"results/topic_models/\"+ id + '.csv'):\n",
    "                df_topic.set_index('origin', inplace=True)\n",
    "                df_topic.to_csv(r\"results/topic_models/\"+ id + '.csv', mode='a', encoding = 'utf8', header=False)\n",
    "            else:\n",
    "                df_topic.set_index('origin', inplace=True)\n",
    "                df_topic.to_csv(r\"results/topic_models/\"+ id + '.csv', encoding = 'utf8')\n",
    "            #pprint(lda_model.print_topics())\n",
    "        else: \n",
    "            print (key + \" is done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting csv file will contain the topics of crossposts.\n",
    "### Sentiment Analysis\n",
    "Sentiment analysis is the computational study of people's emotions expressed in text. In our case we used the popular VADER sentiment analyser, an analyser especially created for social networks (in particular, it was based off Twitter). The result of each comment's analysis will be a number between -1 and 1, depending on whether the comment is perceived as negative or positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(['stopwords', \"vader_lexicon\"]) # Do this the first time you run this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "comments = pd.read_csv('comments.csv', encoding='utf8', on_bad_lines='skip')\n",
    "top_comments = dict()\n",
    "latest_passage = 'results/1st_level\\\\1984isreality.csv @ https://www.reddit.com/r/ukpolitics'\n",
    "for row in comments.iterrows():\n",
    "    if 'reddit' in row[1][1]:\n",
    "        latest_passage = row[1][1]\n",
    "    elif latest_passage not in top_comments.keys():\n",
    "        top_comments[latest_passage] = [row[1][1]]\n",
    "    elif latest_passage in top_comments.keys():\n",
    "        top_comments[latest_passage].append(row[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = pd.read_csv('done.csv', encoding='utf8', on_bad_lines='skip')\n",
    "done_set=set(done.columns)\n",
    "check= set(top_comments.keys())\n",
    "\n",
    "diff = check.difference(done_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from statistics import mean\n",
    "sent = SentimentIntensityAnalyzer()\n",
    "for post in diff:\n",
    "    post_sentiment = [] # median sentiment\n",
    "    data = []\n",
    "    origin, id = post.split('@ ')\n",
    "    if '1st_level_2' in origin:\n",
    "        origin = origin.split('1st_level_2')[1]\n",
    "        origin = origin.replace('\\\\','')\n",
    "        origin = origin.replace('.csv','')\n",
    "    if '1st_level' in origin:\n",
    "        origin = origin.split('1st_level')[1]\n",
    "        origin = origin.replace('\\\\','')\n",
    "        origin = origin.replace('.csv','')\n",
    "    if '2nd_level' in origin:\n",
    "        origin = origin.split('2nd_level')[1]\n",
    "        origin = origin.replace('\\\\','')\n",
    "        origin = origin.replace('.csv','')\n",
    "    id=id[id.index('/r/')+3:]\n",
    "    if 'x0' in post:\n",
    "        post = post.replace('x0','')\n",
    "    mt = top_comments[post]\n",
    "    if len(mt) > 4:\n",
    "        for i in mt:\n",
    "            data.append(reddit.comment(i))\n",
    "        for comment in data:\n",
    "            try:\n",
    "                body = comment.body\n",
    "                if not \"I'm a bot\" in body and not 'I am a bot' in body:\n",
    "                    val = sent.polarity_scores(body)['compound']\n",
    "                    post_sentiment.append(val)\n",
    "            except:\n",
    "                continue\n",
    "        try:\n",
    "            post_sentiment = mean(post_sentiment)\n",
    "        except:\n",
    "            post_sentiment = 0\n",
    "        index = []\n",
    "\n",
    "        sentiment_df = pd.DataFrame({'sentiment': post_sentiment}, index = [origin])\n",
    "        if os.path.isfile(r\"results/sentiment/\"+ id + '.csv'):\n",
    "            sentiment_df.to_csv(r\"results/sentiment/\"+ id + '.csv', mode='a', encoding = 'utf8', header=False)\n",
    "        else:\n",
    "            sentiment_df.to_csv(r\"results/sentiment/\"+ id + '.csv', encoding = 'utf8')\n",
    "        done = pd.read_csv('done.csv', encoding='utf8')\n",
    "        done.insert(0,post,'NaN', allow_duplicates=True)\n",
    "        done.to_csv('done.csv' ,encoding='utf8', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per plottare con pandas etc: inverti colonna, rendi int, droppa colonne inutili, and go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[vedi qui](https://stackabuse.com/rotate-axis-labels-in-matplotlib/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consp = pd.read_csv('results/1st_level/conspiracy.csv', encoding = 'utf8')\n",
    "consp = consp.drop('Unnamed: 0', axis=1)\n",
    "consp = consp.drop('Unnamed: 0.1', axis=1)\n",
    "consp = consp.drop('Unnamed: 0.0.1', axis=1)\n",
    "\n",
    "consp = consp.transpose()\n",
    "consp= consp.drop(0, axis=1)\n",
    "consp = consp.rename({1:'crossposts'}, axis=1)\n",
    "consp = consp.astype(int)\n",
    "consp = consp.sort_values('crossposts', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y=consp.crossposts,x=consp.index)\n",
    "plt.xticks(rotation = 90) # Rotates X-Axis Ticks by 45-degrees\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d65f8af9b1347936d5c0a715a1a101b7602968bee42a1bc2161adfc924f1cbb0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
