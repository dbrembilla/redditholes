{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RedditHoles - A study in  internet Rabbit Holes\n",
    "\n",
    "## An introduction to Reddit\n",
    "\n",
    "Reddit is a social media platform organised in communities, rather than individual connections. This makes the experience on the platform quite different from other Social Networks and closer, in a way, to the one of forums.  \n",
    "\n",
    "### u/User\n",
    "A user (also called redditor, usually referred as “u/” followed by the username) can make a post (also called a submission); posts (also called submissions) can be links, videos, pictures, polls or text. The OP (Original Poster) as well as other users can comment and vote (either upvote or downvote) the post if they find it interesting (so it’s not exactly like like/dislike features of other Social Networks). Finally, users can give posts awards by paying for reddit coins; these recognise the contributions of a post or comment. There are hundreds of these, from a generic gold or silver award to some following internet lingo, such as the “F” award (used ironically to ‘pay respect’). \n",
    "\n",
    "Users can mark their posts as spoilers and multiple types of flairs and markings such as OC (Original Content), Spoilers, +18 and so on. Some subreddits might have rules for posting. \n",
    "\n",
    "Users can be humans or bots. Bots have multiple uses, from auto moderation, to answering with quotes from movies and books, waving flags to other utilities and fun uses. \n",
    "#### The average redditor\n",
    "\n",
    "Reddit users, according to [data from the site itself](https://www.redditinc.com/advertising/audience), are more than 50 million with more than 100 thousand communities. They are mostly male (56%) and have between 18 and 34 year old (58%). \n",
    "\n",
    "### r/Subreddits\n",
    "\n",
    "Reddit is structured in subreddits, communities that group in various ways the interested users. Subreddits can vary from cute pets, to political parties, to recipe advice. Subreddits are usually referred to as “r/” followed by the name of the subreddits (in our case, we will study “r/conspiracy” and related subreddits). \n",
    "\n",
    "Subreddits can vary significantly. They can have rules for posting, different levels of moderation and bot acceptance, all depending from the nature of the subreddit. Rules of subreddits can have different nature; some subreddits may require the content of a post to be marked, some require sources to be linked, some can have no rules at all. This ambiguity makes Reddit a much more decentralised and open Social Network in which some of the cut down on fake news and trolls which happened on sites such as Facebook or Twitter has not yet happened at the same level. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Reddit Data\n",
    "In order to access data from the Reddit API, we used the [`praw` python library](https://github.com/praw-dev/praw). \n",
    "In order to use that, we need:\n",
    "- a Reddit account.\n",
    "- a Reddit app ([here](https://www.reddit.com/prefs/apps) you can create one). Here you'll find the client id under the app name once you create it, as well as the client secret and the user agent, which is your app name.\n",
    "\n",
    "Once we have those, we can access reddit from the python terminal by creating a reddit instance in praw (it is also possible to access as a user by adding username and account if you want to use it in write mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install praw\n",
    "#!pip install pandas\n",
    "import praw\n",
    "reddit = praw.Reddit(client_id = \"CMO4Nf8Dpd3YE_lftmqnHg\", client_secret= \"79YxaeUzQYqtVpvev6SbWbCMvF70-g\", user_agent= \"little_digging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can access information about subreddits, posts and users. As an example, we can access the top 5 posts of all time on Reddit, with the information about the author and the upvote ratio and datat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’ve found a few funny memories during lockdown. This is from my 1st tour in 89, backstage in Vegas. Author: ReallyRickAstley Link: https://www.reddit.com/r/pics/comments/haucpf/ive_found_a_few_funny_memories_during_lockdown/ Subreddit: r/pics Upvote Ratio: 0.99 Date (UTC Format): 1592410647.0\n",
      "Times Square right now Author: SomeGuyInDeutschland Link: https://www.reddit.com/r/wallstreetbets/comments/l8rf4k/times_square_right_now/ Subreddit: r/wallstreetbets Upvote Ratio: 0.99 Date (UTC Format): 1612029638.0\n",
      "Joe Biden elected president of the United States Author: throwawaynumber53 Link: https://www.reddit.com/r/news/comments/jptqj9/joe_biden_elected_president_of_the_united_states/ Subreddit: r/news Upvote Ratio: 0.88 Date (UTC Format): 1604766517.0\n",
      "The Senate. Upvote this so that people see it when they Google \"The Senate\". Author: serventofgaben Link: https://www.reddit.com/r/movies/comments/62sjuh/the_senate_upvote_this_so_that_people_see_it_when/ Subreddit: r/movies Upvote Ratio: 0.96 Date (UTC Format): 1491051474.0\n",
      "My cab driver tonight was so excited to share with me that he’d made the cover of the calendar. I told him I’d help let the world see Author: the_Diva Link: https://www.reddit.com/r/funny/comments/7mjw12/my_cab_driver_tonight_was_so_excited_to_share/ Subreddit: r/funny Upvote Ratio: 0.97 Date (UTC Format): 1514430055.0\n"
     ]
    }
   ],
   "source": [
    "for i in reddit.subreddit('all').top(limit=5): #this will print the top posts all time\n",
    "    print(i.title + ' Author: ' + i.author.name +  ' Link: https://www.reddit.com' + i.permalink + ' Subreddit: r/' + i.subreddit.display_name + ' Upvote Ratio: ' + str(i.upvote_ratio) + ' Date (UTC Format): '+str(i.created_utc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Reddit, starting from r/conspiracy\n",
    "\n",
    "Our objective is to watch how a piece of news or a post is shared between different subreddits. While most social network would measure shares of a post, Reddit is built in a way that if a link is shared in the platform, it is possible to retrieve how much the original link is shared through subreddits. As an example, if an image from imgur is shared, we can use its url to search for the same object on Reddit.\n",
    "This happens because, for the most part, Reddit is used to comment news and multimedia in communities, which often present a common worldview (e.g. subbreddits made by people with same political views).\n",
    "We started with the [r/conspiracy](https://www.reddit.com/r/conspiracy/)'s posts, in particular the top 5000 posts all time. We opted for the top posts all time because the other types of ranking are time-bound and we wanted to watch the overall transmission of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "post_list=list()\n",
    "subreddit_list = list()\n",
    "conspiracy_dict=dict()\n",
    "\n",
    "for i in reddit.subreddit(\"conspiracy\").top(limit=5000): \n",
    "    post_list.append(i.url) # This may seem counterintuitive, but in praw's terms this is the original link of the resource inside the post.\n",
    "\n",
    "\n",
    "for post in post_list:\n",
    "    for repost in reddit.subreddit('all').search('url:'+post): # This function searches for the original post's element\n",
    "        subreddit_url = str(repost.subreddit)\n",
    "        subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "        if subreddit_url in conspiracy_dict.keys():\n",
    "            conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "            conspiracy_dict[subreddit_url][1][0] +=1\n",
    "        else:\n",
    "            conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "            conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "            \n",
    "                \n",
    "\n",
    "\n",
    "df = pd.DataFrame(conspiracy_dict)\n",
    "df.to_csv(r'results/conspiracy_data/conspiracy.csv',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realised after the fact that this method also got the reposts inside the same subreddit. After manually cleaning the csv in this instance, we proceeded to remove this problem in the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a network of subreddits\n",
    "\n",
    "After scraping r/conspiracy, we moved to the neighbouring subreddits. What this will do is creating a network of shared posts between subreddits.\n",
    "\n",
    "First we need to look at all the files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_in_dir(dir):\n",
    "    for filename in os.listdir(dir):\n",
    "        f = os.path.join(dir, filename)\n",
    "        if os.path.isfile(f) and f[-4:] == \".csv\":\n",
    "            yield f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can scrape the other subreddits that had more than 5 posts in common with r/conspiracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('results/conspiracy_data/conspiracy.csv')\n",
    "id_to_analyse = []\n",
    "for column in df1.columns:\n",
    "    value = df1[column][1][1:-1]\n",
    "    if int(value) >= 5:\n",
    "        ind = column.index('/r/')\n",
    "        id = column[ind+3:]\n",
    "        id_to_analyse.append(id)\n",
    "        \n",
    "\n",
    "for subreddit in id_to_analyse:\n",
    "    post_list=list()\n",
    "    subreddit_list = list()\n",
    "    conspiracy_dict=dict() \n",
    "    for i in reddit.subreddit(subreddit).top(limit=5000):\n",
    "        post_list.append((i.title, i.score, i.url))\n",
    "\n",
    "\n",
    "\n",
    "    for post in post_list:\n",
    "        for repost in reddit.subreddit('all').search('url:'+post[2]):\n",
    "            if repost.subreddit_id != \"t5_\"+reddit.subreddit(subreddit).id: #cosa facciamo?\n",
    "                subreddit_url = str(repost.subreddit)\n",
    "                subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "                if subreddit_url in conspiracy_dict.keys():\n",
    "                    conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                    conspiracy_dict[subreddit_url][1][0] +=1\n",
    "                else:\n",
    "                    conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "                    conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "    df = pd.DataFrame(conspiracy_dict)\n",
    "    df.to_csv(r'results/1st_level/'+subreddit+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the cycle one more time, but this time with just the top 500 posts, in order to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "top_comments_list = dict()\n",
    "to_scan = [file for file in get_all_in_dir(\"results/1st_level_2\")]\n",
    "to_avoid= [file.split('\\\\')[1] for file in get_all_in_dir(\"results/1st_level\")]\n",
    "to_avoid.extend(file.split('\\\\')[1] for file in get_all_in_dir(\"results/1st_level_2\"))\n",
    "to_avoid.extend(file.split('\\\\')[1] for file in get_all_in_dir(\"results/2nd_level\"))\n",
    "\n",
    "for subr in to_scan:\n",
    "        id_to_analyse = []\n",
    "        print(f'now opening {subr}')\n",
    "        try:\n",
    "                df1 = pd.read_csv(subr)\n",
    "        except:\n",
    "                pass\n",
    "        try:\n",
    "                df1 = pd.read_csv(subr, encoding='utf8')\n",
    "        except:\n",
    "                print(f'unable to open {subr}')\n",
    "                continue\n",
    "        for column in df1.columns:\n",
    "                try: \n",
    "                        value = df1[column][1][1:-1]\n",
    "                except:\n",
    "                        continue\n",
    "                if int(value) >= 5:\n",
    "                        ind = column.index('/r/')\n",
    "                        id = column[ind+3:]\n",
    "                        if not id + '.csv' in to_avoid:\n",
    "                                try:\n",
    "                                        post_list=list()\n",
    "                                        subreddit_list = list()\n",
    "                                        conspiracy_dict=dict() \n",
    "                                        for i in reddit.subreddit(id).top(limit=500):\n",
    "                                                post_list.append((i.title, i.score, i.url))\n",
    "\n",
    "                                        for post in post_list:\n",
    "                                                for repost in reddit.subreddit('all').search('url:'+post[2]):\n",
    "                                                        if repost.subreddit_id != \"t5_\"+reddit.subreddit(id).id: #cosa facciamo?\n",
    "                                                                subreddit_url = str(repost.subreddit)\n",
    "                                                                subreddit_url = \"https://www.reddit.com/r/\" + subreddit_url\n",
    "                                                        else:\n",
    "                                                                continue\n",
    "                                                        if subreddit_url in conspiracy_dict.keys():\n",
    "                                                                conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                                                                conspiracy_dict[subreddit_url][1][0] +=1\n",
    "                                                        else:\n",
    "                                                                conspiracy_dict[subreddit_url]=[[],[1]]\n",
    "                                                                conspiracy_dict[subreddit_url][0].append(\"https://www.reddit.com\"+repost.permalink)\n",
    "                                        \n",
    "                                        df = pd.DataFrame(conspiracy_dict)\n",
    "                                        df.to_csv(r'results/2nd_level/'+id+'.csv')\n",
    "                                        with open(\"results/2nd_level/done_2.txt\",'a', encoding = \"utf-8\") as text_note:\n",
    "                                                text_note.write(id + \"\\n\")\n",
    "                                                text_note.close()\n",
    "                                        \n",
    "                                except Exception as E:\n",
    "                                        print(E)\n",
    "                                        with open(\"results/2nd_level/error_2.txt\",'a', encoding = \"utf-8\") as text_note:\n",
    "                                                text_note.write(id + \"\\n\")\n",
    "                                                text_note.close()\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "In order to perform more efficiently the operations of data representation we decided two perform two operations:\n",
    "1. We turned the number of crossposts into integers.\n",
    "2. We removed subreddits with less than 5 posts in common.\n",
    "These was in order to have cleaner and more usable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing parentheses and connections with less than 5 reposts\n",
    "\n",
    "datasets= [file for file in get_all_in_dir(\"results/1st_level\")]\n",
    "datasets.extend(file for file in get_all_in_dir(\"results/1st_level_2\"))\n",
    "datasets.extend(file for file in get_all_in_dir(\"results/2nd_level\"))\n",
    "datasets.extend(\"results/conspiracy_data/conspiracy_top_url.csv\")\n",
    "\n",
    "for file in datasets:\n",
    "    try:\n",
    "        df = pd.read_csv(file, on_bad_lines='skip', encoding='utf8')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        df = pd.read_csv(file, on_bad_lines='skip', encoding='latin')\n",
    "    except:\n",
    "        print(file, 'has problems in formatting')\n",
    "        continue\n",
    "    for col in df.columns:\n",
    "        if \"u/\" in col or 'u_' in col: #Sometimes users end up in the columns. Remove them. \n",
    "            df.drop(col, inplace=True, axis=1)\n",
    "        else:\n",
    "            try:\n",
    "                \n",
    "               if isinstance(df[col][1], str): \n",
    "                    try:\n",
    "                        df[col][1]=int(df[col][1][1:-1])\n",
    "                    except:\n",
    "                        continue\n",
    "                    if df[col][1]<5:\n",
    "                        df.drop(col, inplace=True, axis=1)\n",
    "            except:\n",
    "                df.drop(col, inplace=True, axis=1)\n",
    "    df.to_csv(file, encoding=\"utf8\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data from Reddit, we can build a network of shared posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUI DOBBIAMO RIVEDERE L'ORGANIZZAZIONE; deve esserci un csv che contiene i post in comune tra csv1 e csv2, dicendo da dove arrivano. Questo ci permetterà di confrontare i risultanti.\n",
    "## Analysing the comments\n",
    "A part of our inquiry involves the kind of language that redditors use on the website in response to posts. In order to this, we employed two techniques that allow us to find out the nature of a text: Topic Modelling and Sentiment Analysis\n",
    "\n",
    "### Topic modelling\n",
    "Topic modelling is a machine learning technique that tries to predict the distribution of abstract topics in a text, and thus reveal the hidden semantic structures within it. In particular, we used Latent Dirichlet Allocation (LDA) method. We adapted the method used in [here]().\n",
    "The libraries used are <code>[nltk](https://www.nltk.org/), [gensim](https://github.com/RaRe-Technologies/gensim/), [spacy](https://spacy.io/), [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/readme.html)</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "#!pip install gensim\n",
    "#!pip install spacy\n",
    "#!pip install pyLDAvis\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['re', 'edit']) \n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as genmodels  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "#python -m spacy download en_core_web_sm\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of comments can be particularly big, we used this code snippet to get a random sample from bigger comment sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def iterSample(iterable, samplesize):\n",
    "    results = []\n",
    "    for i, v in enumerate(iterable):\n",
    "        r = random.randint(0, i)\n",
    "        if r < samplesize:\n",
    "            if i < samplesize:\n",
    "                results.insert(r, v) # add first samplesize items in random order\n",
    "            else:\n",
    "                results[r] = v # at a decreasing rate, replace random items\n",
    "\n",
    "    if len(results) < samplesize:\n",
    "        raise ValueError(\"Sample larger than population.\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the dataset and the comments from the posts in common between subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received 403 HTTP response\n",
      "iyl433 was not accessible\n",
      "received 403 HTTP response\n",
      "qrknwy was not accessible\n",
      "received 403 HTTP response\n",
      "y4prs was not accessible\n",
      "received 403 HTTP response\n",
      "2seyfu was not accessible\n",
      "received 403 HTTP response\n",
      "9fv8al was not accessible\n",
      "received 403 HTTP response\n",
      "2g37ie was not accessible\n",
      "received 403 HTTP response\n",
      "a72ae4 was not accessible\n",
      "received 403 HTTP response\n",
      "2wyu08 was not accessible\n",
      "received 403 HTTP response\n",
      "amzz75 was not accessible\n",
      "received 403 HTTP response\n",
      "i7uo9a was not accessible\n",
      "received 403 HTTP response\n",
      "39hyor was not accessible\n",
      "received 403 HTTP response\n",
      "3ceysq was not accessible\n",
      "received 403 HTTP response\n",
      "b4s3wv was not accessible\n",
      "received 403 HTTP response\n",
      "b9yulf was not accessible\n",
      "received 403 HTTP response\n",
      "cerc8u was not accessible\n",
      "received 403 HTTP response\n",
      "m6381u was not accessible\n",
      "received 403 HTTP response\n",
      "jgmz1q was not accessible\n",
      "received 403 HTTP response\n",
      "ez1fsy was not accessible\n",
      "received 403 HTTP response\n",
      "1m76pj was not accessible\n",
      "received 403 HTTP response\n",
      "6oz1ap was not accessible\n",
      "received 403 HTTP response\n",
      "2hj4dd was not accessible\n",
      "received 403 HTTP response\n",
      "771z6q was not accessible\n",
      "received 403 HTTP response\n",
      "5l8040 was not accessible\n",
      "received 403 HTTP response\n",
      "sbqwzx was not accessible\n",
      "received 403 HTTP response\n",
      "1g0g2k was not accessible\n",
      "received 403 HTTP response\n",
      "hngm0d was not accessible\n",
      "received 403 HTTP response\n",
      "2ada6i was not accessible\n",
      "received 403 HTTP response\n",
      "1222wm was not accessible\n",
      "received 403 HTTP response\n",
      "f5rtbi was not accessible\n",
      "received 403 HTTP response\n",
      "2oan32 was not accessible\n",
      "received 403 HTTP response\n",
      "gbu41f was not accessible\n",
      "received 403 HTTP response\n",
      "ry9vfu was not accessible\n",
      "received 403 HTTP response\n",
      "avfx7e was not accessible\n",
      "received 403 HTTP response\n",
      "88mhnv was not accessible\n",
      "received 403 HTTP response\n",
      "73638v was not accessible\n",
      "received 403 HTTP response\n",
      "2gge38 was not accessible\n",
      "received 403 HTTP response\n",
      "jkp9at was not accessible\n",
      "received 403 HTTP response\n",
      "1ghapr was not accessible\n",
      "received 403 HTTP response\n",
      "phqij7 was not accessible\n",
      "received 403 HTTP response\n",
      "2ligpn was not accessible\n",
      "received 403 HTTP response\n",
      "lrrvu9 was not accessible\n",
      "received 403 HTTP response\n",
      "a5y3rm was not accessible\n",
      "received 403 HTTP response\n",
      "gj84id was not accessible\n",
      "received 403 HTTP response\n",
      "awk9b7 was not accessible\n",
      "received 403 HTTP response\n",
      "6pa9ru was not accessible\n",
      "received 403 HTTP response\n",
      "6p9mn7 was not accessible\n",
      "received 403 HTTP response\n",
      "6jqsp1 was not accessible\n",
      "received 403 HTTP response\n",
      "51wkad was not accessible\n",
      "received 403 HTTP response\n",
      "2dmdba was not accessible\n",
      "received 403 HTTP response\n",
      "733dlz was not accessible\n",
      "received 403 HTTP response\n",
      "2x13ht was not accessible\n",
      "received 403 HTTP response\n",
      "532256 was not accessible\n",
      "received 403 HTTP response\n",
      "6paf4y was not accessible\n",
      "received 403 HTTP response\n",
      "ku1hmh was not accessible\n",
      "received 403 HTTP response\n",
      "jf5vm1 was not accessible\n",
      "received 403 HTTP response\n",
      "52lks7 was not accessible\n",
      "received 403 HTTP response\n",
      "2vrioo was not accessible\n",
      "received 403 HTTP response\n",
      "ca0pep was not accessible\n",
      "received 403 HTTP response\n",
      "6se9xj was not accessible\n",
      "received 403 HTTP response\n",
      "kdrykq was not accessible\n",
      "received 403 HTTP response\n",
      "o3znx2 was not accessible\n",
      "received 403 HTTP response\n",
      "np3i22 was not accessible\n",
      "received 403 HTTP response\n",
      "esexut was not accessible\n",
      "received 403 HTTP response\n",
      "52950h was not accessible\n",
      "received 403 HTTP response\n",
      "fmjk3r was not accessible\n",
      "received 403 HTTP response\n",
      "edf36m was not accessible\n",
      "received 403 HTTP response\n",
      "gps4ji was not accessible\n",
      "received 403 HTTP response\n",
      "fhnj2b was not accessible\n",
      "received 403 HTTP response\n",
      "jgmz1q was not accessible\n",
      "received 403 HTTP response\n",
      "hgr8zi was not accessible\n",
      "received 403 HTTP response\n",
      "fj8yyz was not accessible\n",
      "received 403 HTTP response\n",
      "pdcdny was not accessible\n",
      "received 403 HTTP response\n",
      "gr5kql was not accessible\n",
      "received 403 HTTP response\n",
      "1m76pj was not accessible\n",
      "received 403 HTTP response\n",
      "2hj4dd was not accessible\n",
      "received 403 HTTP response\n",
      "mh4our was not accessible\n",
      "received 403 HTTP response\n",
      "o61at6 was not accessible\n",
      "received 403 HTTP response\n",
      "gu403z was not accessible\n",
      "received 403 HTTP response\n",
      "mv5uk6 was not accessible\n",
      "received 403 HTTP response\n",
      "iyl433 was not accessible\n",
      "received 403 HTTP response\n",
      "hcj2xy was not accessible\n",
      "received 403 HTTP response\n",
      "g9m9k4 was not accessible\n",
      "received 403 HTTP response\n",
      "ewxfcg was not accessible\n",
      "received 403 HTTP response\n",
      "6oz1ap was not accessible\n",
      "received 403 HTTP response\n",
      "2x89yo was not accessible\n",
      "received 403 HTTP response\n",
      "2hj4dd was not accessible\n",
      "received 403 HTTP response\n",
      "j9lrq5 was not accessible\n",
      "received 403 HTTP response\n",
      "1g0g2k was not accessible\n",
      "received 403 HTTP response\n",
      "9tlwkx was not accessible\n",
      "received 403 HTTP response\n",
      "2hj4dd was not accessible\n",
      "received 403 HTTP response\n",
      "nuq6uw was not accessible\n",
      "received 403 HTTP response\n",
      "jkp9at was not accessible\n",
      "received 403 HTTP response\n",
      "f4zrec was not accessible\n",
      "received 403 HTTP response\n",
      "t9e9ts was not accessible\n",
      "received 403 HTTP response\n",
      "oenar4 was not accessible\n",
      "received 403 HTTP response\n",
      "6jqsp1 was not accessible\n",
      "received 403 HTTP response\n",
      "mf3bxj was not accessible\n",
      "received 403 HTTP response\n",
      "fnbhv4 was not accessible\n",
      "received 403 HTTP response\n",
      "gt99s0 was not accessible\n",
      "received 403 HTTP response\n",
      "9bdp5f was not accessible\n",
      "received 403 HTTP response\n",
      "2wm71f was not accessible\n",
      "received 403 HTTP response\n",
      "nboklc was not accessible\n",
      "received 403 HTTP response\n",
      "kuoajg was not accessible\n",
      "received 403 HTTP response\n",
      "nbokkw was not accessible\n",
      "received 403 HTTP response\n",
      "7invgh was not accessible\n",
      "received 403 HTTP response\n",
      "6pcb0a was not accessible\n",
      "received 403 HTTP response\n",
      "kjw4rc was not accessible\n",
      "received 403 HTTP response\n",
      "pfmp1m was not accessible\n",
      "received 403 HTTP response\n",
      "8n1wvd was not accessible\n",
      "received 403 HTTP response\n",
      "o1ojxd was not accessible\n",
      "received 403 HTTP response\n",
      "c156kp was not accessible\n",
      "received 403 HTTP response\n",
      "f4u9hj was not accessible\n",
      "received 403 HTTP response\n",
      "2vrcpn was not accessible\n",
      "received 403 HTTP response\n",
      "50vair was not accessible\n",
      "received 403 HTTP response\n",
      "jemix0 was not accessible\n",
      "received 403 HTTP response\n",
      "kmswhu was not accessible\n",
      "received 403 HTTP response\n",
      "pwoskl was not accessible\n",
      "received 403 HTTP response\n",
      "jkp9at was not accessible\n",
      "received 403 HTTP response\n",
      "ack1io was not accessible\n",
      "received 403 HTTP response\n",
      "bbuev6 was not accessible\n",
      "received 403 HTTP response\n",
      "e2igz8 was not accessible\n",
      "received 403 HTTP response\n",
      "d394u1 was not accessible\n",
      "received 403 HTTP response\n",
      "nsv1zi was not accessible\n",
      "received 403 HTTP response\n",
      "nb2lpv was not accessible\n",
      "received 403 HTTP response\n",
      "7tefkc was not accessible\n",
      "received 403 HTTP response\n",
      "qjqbia was not accessible\n",
      "received 403 HTTP response\n",
      "nihscq was not accessible\n",
      "received 403 HTTP response\n",
      "ktd127 was not accessible\n",
      "received 403 HTTP response\n",
      "ehuisq was not accessible\n",
      "received 403 HTTP response\n",
      "fmf7xg was not accessible\n",
      "received 403 HTTP response\n",
      "pcalam was not accessible\n",
      "received 403 HTTP response\n",
      "gu403z was not accessible\n",
      "received 403 HTTP response\n",
      "rwpb30 was not accessible\n",
      "received 403 HTTP response\n",
      "qakfuy was not accessible\n",
      "received 403 HTTP response\n",
      "i5gyvo was not accessible\n",
      "received 403 HTTP response\n",
      "kmi85r was not accessible\n",
      "received 403 HTTP response\n",
      "lyrnsi was not accessible\n",
      "received 403 HTTP response\n",
      "jke18q was not accessible\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "to_scan= [file for file in get_all_in_dir(\"results/1st_level\")]\n",
    "to_scan.extend(file for file in get_all_in_dir(\"results/1st_level_2\"))\n",
    "to_scan.extend(file for file in get_all_in_dir(\"results/2nd_level\"))\n",
    "top_comments_list = dict()\n",
    "for subreddit in to_scan:\n",
    "        try: \n",
    "                df = pd.read_csv(subreddit, sep=',', encoding='utf8', on_bad_lines='skip')\n",
    "        except:\n",
    "                print(subreddit + ' is a bad boy')\n",
    "        done = pd.read_csv('done.csv', keep_default_na=False, na_values=[\"\"])\n",
    "                \n",
    "        done_col = set(done.columns)\n",
    "        cols = set(df.columns[1:])\n",
    "        cols = cols.difference(done_col)\n",
    "        for column in cols: # qui inserire un loop sui permalink\n",
    "                if column not in done_col and 'Unnamed' not in column:\n",
    "                        \n",
    "                        list_post=[]\n",
    "                        list_post= df[column][0].split(',')\n",
    "                        name= subreddit + \" @ \" + column\n",
    "                        for post_url in list_post:\n",
    "                                if post_url == 0:\n",
    "                                        continue\n",
    "                        \n",
    "                                #tmp =  re.sub(\"\\[\\]\\'\", \"\", post_url) NON FUNZIONA E NON CAPISCO PERCHé\n",
    "                                if '[' in post_url:\n",
    "                                        post_url = post_url.replace('[','')\n",
    "                                if ']' in post_url:\n",
    "                                        post_url = post_url.replace(']','')\n",
    "                                try:\n",
    "                                        post = reddit.submission(url=post_url[1:-1])\n",
    "                                except Exception as e: # some posts may be removed\n",
    "                                        continue\n",
    "                                try:\n",
    "                                        #This allows up to 1 reply to each post.\n",
    "                                        post.comments.replace_more(limit=1)\n",
    "                                except Exception as e:\n",
    "                                        print(e)\n",
    "                                        print(f'{post} was not accessible')\n",
    "                                        continue\n",
    "                                result = []\n",
    "                                if len(post.comments)> 500:\n",
    "                                        for comment in iterSample(post.comments, 500):\n",
    "                                                #result.append(comment)\n",
    "                                                result.extend(comment.replies.list())\n",
    "                                else:\n",
    "                                        for comment in post.comments:\n",
    "                                                #result.append(comment)\n",
    "                                                result.extend(comment.replies.list())\n",
    "\n",
    "\n",
    "                                if name in top_comments_list.keys():\n",
    "                                        top_comments_list[name].extend(result)\n",
    "                                else:\n",
    "                                        top_comments_list[name]=result\n",
    "                        try:\n",
    "                                val = top_comments_list[name]\n",
    "                        except KeyError:\n",
    "                                continue\n",
    "                        res=pd.DataFrame({name:val})\n",
    "                        res.to_csv('comments.csv',mode='a')\n",
    "                        \n",
    "                        done.insert(0,column,'NaN')\n",
    "                        done.to_csv('done.csv' , mode='w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform the topic modeling: first we need to define the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print a csv for each crossposting between subreddits with the topic modeling of the common posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('comments.csv', encoding='utf8', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_comments = dict()\n",
    "latest_passage = 'results/1st_level\\\\1984isreality.csv @ https://www.reddit.com/r/ukpolitics'\n",
    "for row in comments.iterrows():\n",
    "    if 'reddit' in row[1][1]:\n",
    "        latest_passage = row[1][1]\n",
    "    elif latest_passage not in top_comments.keys():\n",
    "        top_comments[latest_passage] = [row[1][1]]\n",
    "    elif latest_passage in top_comments.keys():\n",
    "        top_comments[latest_passage].append(row[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14624/2852605259.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'done.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_bad_lines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'skip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdone_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_comments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "done = pd.read_csv('done.csv', encoding='utf8', on_bad_lines='skip')\n",
    "done_set=set(done.columns)\n",
    "check= set(top_comments.keys())\n",
    "\n",
    "diff = set(check).difference(done_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cryptoandme\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             conn = connection.create_connection(\n\u001b[0m\u001b[0;32m    175\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13368/2554767873.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Remove tags from comments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\S*@\\S*\\s?'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Remove new line characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13368/2554767873.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Remove tags from comments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\S*@\\S*\\s?'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Remove new line characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\praw\\models\\reddit\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attribute)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;34m\"\"\"Return the value of `attribute`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetched\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         raise AttributeError(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\praw\\models\\reddit\\comment.py\u001b[0m in \u001b[0;36m_fetch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\praw\\models\\reddit\\comment.py\u001b[0m in \u001b[0;36m_fetch_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAPI_PATH\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GET\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\praw\\reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, path, params, data, files, json)\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mClientException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"At most one of `data` and `json` is supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m             return self._core.request(\n\u001b[0m\u001b[0;32m    886\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m             \u001b[0mjson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"api_type\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moauth_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         return self._request_with_retries(\n\u001b[0m\u001b[0;32m    331\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mretry_strategy_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         response, saved_exception = self._make_request(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[0;32m    183\u001b[0m     ):\n\u001b[0;32m    184\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             response = self._rate_limiter.call(\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_header_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\prawcore\\rate_limit.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \"\"\"\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"headers\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset_header_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36m_set_header_callback\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_authorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"refresh\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ):\n\u001b[1;32m--> 283\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_authorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"Authorization\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mf\"bearer {self._authorizer.access_token}\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\prawcore\\auth.py\u001b[0m in \u001b[0;36mrefresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scopes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m             \u001b[0madditional_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"scope\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scopes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m         self._request_token(\n\u001b[0m\u001b[0;32m    374\u001b[0m             \u001b[0mgrant_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"client_credentials\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0madditional_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\prawcore\\auth.py\u001b[0m in \u001b[0;36m_request_token\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    153\u001b[0m         )\n\u001b[0;32m    154\u001b[0m         \u001b[0mpre_request_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_authenticator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[0mpayload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpayload\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Why are these OKAY responses?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\prawcore\\auth.py\u001b[0m in \u001b[0;36m_post\u001b[1;34m(self, url, success_status, **data)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_post\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuccess_status\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ok\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         response = self._requestor.request(\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\prawcore\\requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;34m\"\"\"Issue the HTTP request capturing any errors that may occur.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             return self._http.request(\n\u001b[0m\u001b[0;32m     59\u001b[0m                 \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \u001b[1;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[0mtls_in_tls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             conn = connection.create_connection(\n\u001b[0m\u001b[0;32m    175\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m             )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Refactor for\n",
    "for key in diff:\n",
    "    origin, id = key.split('@ ')\n",
    "    idx = id.index('/r/')+3\n",
    "    id=id[idx:]\n",
    "    if 'x0' in key:\n",
    "        key = key.replace('x0','')\n",
    "\n",
    "    # Convert to list\n",
    "    data = []\n",
    "    mt = top_comments[key]\n",
    "    if len(mt) > 4:\n",
    "        for i in mt:\n",
    "            data.append(reddit.comment(i))\n",
    "        # Remove tags from comments\n",
    "        data = [re.sub('\\S*@\\S*\\s?', '', sent.body) for sent in data]\n",
    "\n",
    "        # Remove new line characters\n",
    "        data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "        # Remove distracting single quotes\n",
    "        data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "        # Tokenizing the text\n",
    "        data_words = list(sent_to_words(data))\n",
    "\n",
    "        # Build the bigram and trigram models\n",
    "        bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "        trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "        # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "        bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "        trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "        # Remove Stop Words\n",
    "        data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "        # Form Bigrams\n",
    "        data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "        # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "        # python3 -m spacy download en\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "        # Do lemmatization keeping only noun, adj, vb, adv\n",
    "        data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "        # Create Dictionary\n",
    "        id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "        # Create Corpus\n",
    "        texts = data_lemmatized\n",
    "\n",
    "        # Term Document Frequency\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "        # Human readable format of corpus (term-frequency)\n",
    "        [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "        if corpus != [] and corpus != [[]]:\n",
    "            # Build LDA model\n",
    "            lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                    id2word=id2word,\n",
    "                                                    num_topics=10, \n",
    "                                                    random_state=100,\n",
    "                                                    update_every=1,\n",
    "                                                    chunksize=100,\n",
    "                                                    passes=10,\n",
    "                                                    alpha='auto',\n",
    "                                                    per_word_topics=True)\n",
    "\n",
    "            # Print the Keyword in the 10 topics\n",
    "            print('check2', key)\n",
    "            try:\n",
    "                origin= origin.split('\\\\')[1]\n",
    "            except:\n",
    "                origin = origin.split('level')[1]\n",
    "            index = []\n",
    "            topics = {}\n",
    "            list_topics=lda_model.print_topics()\n",
    "            for i in list_topics:\n",
    "                for value in range(len(list_topics)):\n",
    "                    to_add = []\n",
    "                    for el in list_topics[value][1].split('+'):\n",
    "                        start = el.index('\"')\n",
    "                        word = el[start+1:-2]             \n",
    "                        to_add.append(word)\n",
    "                    topics[value] = to_add\n",
    "\n",
    "            \n",
    "            df_topic = pd.DataFrame(topics)\n",
    "            done = pd.read_csv('done.csv', encoding='utf8')\n",
    "            done.insert(0,key,'NaN')\n",
    "            done.to_csv('done.csv' ,encoding='utf8', mode='w')\n",
    "\n",
    "            for row in df_topic.index:\n",
    "                index.append(origin)\n",
    "            df_topic['origin'] = index\n",
    "            if os.path.isfile(r\"results/topic_models/\"+ id + '.csv'):\n",
    "                df_topic.to_csv(r\"results/topic_models/\"+ id + '.csv', mode='a')\n",
    "            else:\n",
    "                df_topic.set_index('origin', inplace=True)\n",
    "                df_topic.to_csv(r\"results/topic_models/\"+ id + '.csv')\n",
    "            #pprint(lda_model.print_topics())\n",
    "        else: \n",
    "            print (key + \" is done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting csv file will contain the topics of crossposts.\n",
    "# RIFAI CONTANDO DA DOVE PARTE\n",
    "### Sentiment Analysis\n",
    "Sentiment analysis is the computational study of people's emotions expressed in text. In our case we used the popular VADER sentiment analyser, an analyser especially created for social networks (in particular, it was based off Twitter). The result of each comment's analysis will be a number between -1 and 1, depending on whether the comment is perceived as negative or positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(['stopwords', \"vader_lexicon\"]) # Do this the first time you run this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from statistics import mean\n",
    "sent = SentimentIntensityAnalyzer()\n",
    "sentiment_values = list()\n",
    "for post in top_comments_list:\n",
    "    post_sentiment = [] # median sentiment\n",
    "\n",
    "    for comment in post['comments']:\n",
    "        body = comment.body\n",
    "        if not \"I'm a bot\" in body and not 'I am a bot' in body:\n",
    "            for reply in comment.replies:\n",
    "                if not \"I'm a bot\" in reply.body and not 'I am a bot' in reply.body:\n",
    "                    body += reply.body\n",
    "                for second_reply in reply.replies:\n",
    "                    if not \"I'm a bot\" in second_reply.body and not 'I am a bot' in second_reply.body:\n",
    "                        body += second_reply.body\n",
    "            val = sent.polarity_scores(body)['compound']\n",
    "            post_sentiment.append(val)\n",
    "    try:\n",
    "        post_sentiment = mean(post_sentiment)\n",
    "    except:\n",
    "        post_sentiment = 0\n",
    "    sentiment_values.append({'sentiment': post_sentiment,'permalink': post['permalink'],'posted_in': post['target'], 'posted_from':post['posted_from']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.DataFrame(sentiment_values, index='posted_in')\n",
    "sentiment_df.to_csv('results/sentiment/sentiment.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d65f8af9b1347936d5c0a715a1a101b7602968bee42a1bc2161adfc924f1cbb0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
